{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"About AffectiveTweets is a WEKA package for analyzing emotion and sentiment of tweets. The source code is hosted on Github . The package implements WEKA filters for calculating state-of-the-art affective analysis features from tweets that can be fed into machine learning algorithms. Many of these features were drawn from the NRC-Canada System . It also implements methods for building affective lexicons and distant supervision methods for training affective models from unlabelled tweets. Description about the filters, installation instructions, and examples are given below. Official Baseline System The package was made available as the official baseline system for the WASSA-2017 Shared Task on Emotion Intensity (EmoInt) and for SemEval-2018 Task 1: Affect in Tweets . Five participating teams used AffectiveTweets in WASSA-2017 to generate feature vectors, including the teams that eventually ranked first, second, and third. For SemEval-2018, the package was used by 15 teams. Relevant Papers The most relevant papers on which this package is based are: Sentiment Analysis of Short Informal Texts . Svetlana Kiritchenko, Xiaodan Zhu and Saif Mohammad. Journal of Artificial Intelligence Research, volume 50, pages 723-762, August 2014. BibTeX Meta-Level Sentiment Models for Big Social Data Analysis . F. Bravo-Marquez, M. Mendoza and B. Poblete. Knowledge-Based Systems Volume 69, October 2014, Pages 86\u201399. BibTex Stance and sentiment in tweets . Saif M. Mohammad, Parinaz Sobhani, and Svetlana Kiritchenko. 2017. Special Section of the ACM Transactions on Internet Technology on Argumentation in Social Media 17(3). BibTeX Sentiment strength detection for the social Web . Thelwall, M., Buckley, K., & Paltoglou, G. (2012). Journal of the American Society for Information Science and Technology, 63(1), 163-173. BibTex Citation Please cite the following paper if using this package in an academic publication: F. Bravo-Marquez, E. Frank, B. Pfahringer, and S. M. Mohammad AffectiveTweets: a WEKA Package for Analyzing Affect in Tweets , In Journal of Machine Learning Research Volume 20(92), pages 1\u22126, 2019. ( pdf ) You are also welcome to cite a previous publication describing the package: S. M. Mohammad and F. Bravo-Marquez Emotion Intensities in Tweets , In * Sem '17: Proceedings of the sixth joint conference on lexical and computational semantics (*Sem) , August 2017, Vancouver, Canada. ( pdf ) You should also cite the papers describing any of the lexicons or resources you are using with this package. Here is the BibTex entry for the package along with the entries for the resources listed below. Here is the BibTex entry just for the package. The individual references for each resource can be found through the links provided below. Filters Tweet-level Filters TweetToSparseFeatureVector : calculates sparse features, such as word and character n-grams from tweets. There are parameters for filtering out infrequent features e.g., (n-grams occurring in less than m tweets) and for setting the weighting approach (boolean or frequency based). Word n-grams : extracts word n-grams from n =1 to a maximum value. Negations : add a prefix to words occurring in negated contexts, e.g., I don't like you => I don't NEG-like NEG-you. The prefixes only affect word n-gram features. The scope of negation finishes with the next punctuation expression ([\\.|,|:|;|!|\\?]+) . Character n-grams : calculates character n-grams. POS tags : tags tweets using the CMU Tweet NLP tool , and creates a vector space model based on the sequence of POS tags. BibTex Brown clusters : maps the words in a tweet to Brown word clusters and creates a low-dimensional vector space model. It can be used with n-grams of word clusters. The word clusters are also taken from the CMU Tweet NLP tool . TweetToLexiconFeatureVector : calculates features from a tweet using several lexicons. MPQA : counts the number of positive and negative words from the MPQA subjectivity lexicon. BibTex Bing Liu : counts the number of positive and negative words from the Bing Liu lexicon. BibTex AFINN : calculates positive and negative variables by aggregating the positive and negative word scores provided by this lexicon. BibTex Sentiment140 : calculates positive and negative variables by aggregating the positive and negative word scores provided by this lexicon created with tweets annotated by emoticons. BibTex NRC Hashtag Sentiment lexicon : calculates positive and negative variables by aggregating the positive and negative word scores provided by this lexicon created with tweets annotated with emotional hashtags. BibTex NRC Word-Emotion Association Lexicon : counts the number of words matching each emotion from this lexicon. BibTex NRC-10 Expanded : adds the emotion associations of the words matching the Twitter Specific expansion of the NRC Word-Emotion Association Lexicon. BibTex NRC Hashtag Emotion Association Lexicon : adds the emotion associations of the words matching this lexicon. BibTex SentiWordNet : calculates positive and negative scores using SentiWordnet. We calculate a weighted average of the sentiment distributions of the synsets for word occurring in multiple synsets. The weights correspond to the reciprocal ranks of the senses in order to give higher weights to most popular senses. BibTex Emoticons : calculates a positive and a negative score by aggregating the word associations provided by a list of emoticons. The list is taken from the AFINN project. Negations: counts the number of negating words in the tweet. TweetToInputLexiconFeatureVector : calculates features from a tweet using a given list of affective lexicons, where each lexicon is represented as an ARFF file. The features are calculated by adding or counting the affective associations of the words matching the given lexicons. All numeric and nominal attributes from each lexicon are considered. Numeric scores are added and nominal are counted. The NRC-Affect-Intensity lexicon is used by deault. BibTex TweetToSentiStrengthFeatureVector : calculates positive and negative sentiment strengths for a tweet using SentiStrength . Disclaimer: SentiStrength can only be used for academic purposes from within this package. BibTex TweetToEmbeddingsFeatureVector : calculates a tweet-level feature representation using pre-trained word embeddings. A dummy word-embedding formed by zeroes is used for word with no corresponding embedding. The tweet vectors can be calculated using the following schemes: Average word embeddings. Add word embeddings. Concatenation of first k embeddings. Dummy values are added if the tweet has less than k words. TweetNLPPOSTagger : runs the Twitter-specific POS tagger from the CMU TweetNLP library on the given tweets. POS tags are prepended to the tokens. Word-level Filters PMILexiconExpander : calculates the Pointwise Mutual Information (PMI) semantic orientation for each word in a corpus of tweets annotated by sentiment. The score is calculated by subtracting the PMI of the target word with a negative sentiment from the PMI of the target word with a positive sentiment. This is a supervised filter. BibTex TweetCentroid : calculates word distributional vectors from a corpus of unlabelled tweets by treating them as the centroid of the tweet vectors in which they appear. The vectors can be labelled using an affective lexicon to train a word-level affective classifier. This classifier can be used to expand the original lexicon. BibTex , original paper LabelWordVectors : labels word vectors with an input lexicon in arff format. This filter is useful for training word-level affective classifiers. Distant Supervision Filters ASA : Annotate-Sample-Average (ASA) is a lexicon-based distant supervision method for training polarity classifiers in Twitter in the absence of labelled data. It takes a collection of unlabelled tweets and a polarity lexicon in arff format and creates synthetic labelled instances. Each labelled instance is created by sampling with replacement a number of tweets containing at least one word from the lexicon with the desired polarity, and averaging the feature vectors of the sampled tweets. BibTex , original paper PTCM : The Partitioned Tweet Centroid Model (PTCM) is an adaption of the TweetCentroidModel for distant supervision. As tweets and words are represented by the same feature vectors, a word-level classifier trained from a polarity lexicon and a corpus of unlabelled tweets can be used for classifying the sentiment of tweets represented by sparse feature vectors. In other words, the labelled word vectors correspond to lexicon-annotated training data for message-level polarity classification. The model includes a simple modification to the tweet centroid model for increasing the number of labelled instances, yielding partitioned tweet centroids . This modification is based on partitioning the tweets associated with each word into smaller disjoint subsets of a fixed size. The method calculates one centroid per partition, which is labelled according to the lexicon. BibTex , original paper LexiconDistantSupervision : This is the most popular distant supervision approach for Twitter sentiment analysis. It takes a collection of unlabelled tweets and a polarity lexicon in arff format of positive and negative tokens. If a word from the lexicon is found, the tweet is labelled with the word's polarity. Tweets with both positive and negative words are discarded. The word used for labelling the tweet can be removed from the content. Emoticons are used as the default lexicon. original paper Tokenizers TweetNLPTokenizer : a Twitter-specific String tokenizer based on the CMU Tweet NLP tool that can be used with the existing StringWordToVector Weka filter. Other Resources Datasets : The package provides some tweets annotated by affective values in gzipped ARFF format in $WEKA_HOME/packages/AffectiveTweets/data/. The default location for $WEKA_HOME is $HOME/wekafiles. Affective Lexicons : The package provides affective lexicons in ARFF format. These lexicons are located in $WEKA_HOME/packages/AffectiveTweets/lexicons/arff_lexicons/ and can be used with the TweetToInputLexiconFeatureVector filter. Pre-trained Word-Embeddings : The package provides a file with pre-trained word vectors trained with the Word2Vec tool in gzip compressed format. It is a tab separated file with the word in last column located in $WEKA_HOME/packages/AffectiveTweets/resources/w2v.twitter.edinburgh.100d.csv.gz. However, this is a toy example trained from a small collection of tweets. We recommend downloading w2v.twitter.edinburgh10M.400d.csv.gz , which provides embeddings trained from 10 million tweets taken from the Edinburgh corpus . The parameters were calibrated for classifying words into emotions. More info in this paper . Documentation The Java documentation is available here . Team Main Developer Felipe Bravo-Marquez Contributors Saif Mohammad Eibe Frank Bernhard Pfahringer Contact Email: fbravo at dcc.uchile.cl If you have questions about Weka please refer to the Weka mailing list .","title":"Home"},{"location":"#about","text":"AffectiveTweets is a WEKA package for analyzing emotion and sentiment of tweets. The source code is hosted on Github . The package implements WEKA filters for calculating state-of-the-art affective analysis features from tweets that can be fed into machine learning algorithms. Many of these features were drawn from the NRC-Canada System . It also implements methods for building affective lexicons and distant supervision methods for training affective models from unlabelled tweets. Description about the filters, installation instructions, and examples are given below.","title":"About"},{"location":"#official-baseline-system","text":"The package was made available as the official baseline system for the WASSA-2017 Shared Task on Emotion Intensity (EmoInt) and for SemEval-2018 Task 1: Affect in Tweets . Five participating teams used AffectiveTweets in WASSA-2017 to generate feature vectors, including the teams that eventually ranked first, second, and third. For SemEval-2018, the package was used by 15 teams.","title":"Official Baseline System"},{"location":"#relevant-papers","text":"The most relevant papers on which this package is based are: Sentiment Analysis of Short Informal Texts . Svetlana Kiritchenko, Xiaodan Zhu and Saif Mohammad. Journal of Artificial Intelligence Research, volume 50, pages 723-762, August 2014. BibTeX Meta-Level Sentiment Models for Big Social Data Analysis . F. Bravo-Marquez, M. Mendoza and B. Poblete. Knowledge-Based Systems Volume 69, October 2014, Pages 86\u201399. BibTex Stance and sentiment in tweets . Saif M. Mohammad, Parinaz Sobhani, and Svetlana Kiritchenko. 2017. Special Section of the ACM Transactions on Internet Technology on Argumentation in Social Media 17(3). BibTeX Sentiment strength detection for the social Web . Thelwall, M., Buckley, K., & Paltoglou, G. (2012). Journal of the American Society for Information Science and Technology, 63(1), 163-173. BibTex","title":"Relevant Papers"},{"location":"#citation","text":"Please cite the following paper if using this package in an academic publication: F. Bravo-Marquez, E. Frank, B. Pfahringer, and S. M. Mohammad AffectiveTweets: a WEKA Package for Analyzing Affect in Tweets , In Journal of Machine Learning Research Volume 20(92), pages 1\u22126, 2019. ( pdf ) You are also welcome to cite a previous publication describing the package: S. M. Mohammad and F. Bravo-Marquez Emotion Intensities in Tweets , In * Sem '17: Proceedings of the sixth joint conference on lexical and computational semantics (*Sem) , August 2017, Vancouver, Canada. ( pdf ) You should also cite the papers describing any of the lexicons or resources you are using with this package. Here is the BibTex entry for the package along with the entries for the resources listed below. Here is the BibTex entry just for the package. The individual references for each resource can be found through the links provided below.","title":"Citation"},{"location":"#filters","text":"","title":"Filters"},{"location":"#tweet-level-filters","text":"TweetToSparseFeatureVector : calculates sparse features, such as word and character n-grams from tweets. There are parameters for filtering out infrequent features e.g., (n-grams occurring in less than m tweets) and for setting the weighting approach (boolean or frequency based). Word n-grams : extracts word n-grams from n =1 to a maximum value. Negations : add a prefix to words occurring in negated contexts, e.g., I don't like you => I don't NEG-like NEG-you. The prefixes only affect word n-gram features. The scope of negation finishes with the next punctuation expression ([\\.|,|:|;|!|\\?]+) . Character n-grams : calculates character n-grams. POS tags : tags tweets using the CMU Tweet NLP tool , and creates a vector space model based on the sequence of POS tags. BibTex Brown clusters : maps the words in a tweet to Brown word clusters and creates a low-dimensional vector space model. It can be used with n-grams of word clusters. The word clusters are also taken from the CMU Tweet NLP tool . TweetToLexiconFeatureVector : calculates features from a tweet using several lexicons. MPQA : counts the number of positive and negative words from the MPQA subjectivity lexicon. BibTex Bing Liu : counts the number of positive and negative words from the Bing Liu lexicon. BibTex AFINN : calculates positive and negative variables by aggregating the positive and negative word scores provided by this lexicon. BibTex Sentiment140 : calculates positive and negative variables by aggregating the positive and negative word scores provided by this lexicon created with tweets annotated by emoticons. BibTex NRC Hashtag Sentiment lexicon : calculates positive and negative variables by aggregating the positive and negative word scores provided by this lexicon created with tweets annotated with emotional hashtags. BibTex NRC Word-Emotion Association Lexicon : counts the number of words matching each emotion from this lexicon. BibTex NRC-10 Expanded : adds the emotion associations of the words matching the Twitter Specific expansion of the NRC Word-Emotion Association Lexicon. BibTex NRC Hashtag Emotion Association Lexicon : adds the emotion associations of the words matching this lexicon. BibTex SentiWordNet : calculates positive and negative scores using SentiWordnet. We calculate a weighted average of the sentiment distributions of the synsets for word occurring in multiple synsets. The weights correspond to the reciprocal ranks of the senses in order to give higher weights to most popular senses. BibTex Emoticons : calculates a positive and a negative score by aggregating the word associations provided by a list of emoticons. The list is taken from the AFINN project. Negations: counts the number of negating words in the tweet. TweetToInputLexiconFeatureVector : calculates features from a tweet using a given list of affective lexicons, where each lexicon is represented as an ARFF file. The features are calculated by adding or counting the affective associations of the words matching the given lexicons. All numeric and nominal attributes from each lexicon are considered. Numeric scores are added and nominal are counted. The NRC-Affect-Intensity lexicon is used by deault. BibTex TweetToSentiStrengthFeatureVector : calculates positive and negative sentiment strengths for a tweet using SentiStrength . Disclaimer: SentiStrength can only be used for academic purposes from within this package. BibTex TweetToEmbeddingsFeatureVector : calculates a tweet-level feature representation using pre-trained word embeddings. A dummy word-embedding formed by zeroes is used for word with no corresponding embedding. The tweet vectors can be calculated using the following schemes: Average word embeddings. Add word embeddings. Concatenation of first k embeddings. Dummy values are added if the tweet has less than k words. TweetNLPPOSTagger : runs the Twitter-specific POS tagger from the CMU TweetNLP library on the given tweets. POS tags are prepended to the tokens.","title":"Tweet-level Filters"},{"location":"#word-level-filters","text":"PMILexiconExpander : calculates the Pointwise Mutual Information (PMI) semantic orientation for each word in a corpus of tweets annotated by sentiment. The score is calculated by subtracting the PMI of the target word with a negative sentiment from the PMI of the target word with a positive sentiment. This is a supervised filter. BibTex TweetCentroid : calculates word distributional vectors from a corpus of unlabelled tweets by treating them as the centroid of the tweet vectors in which they appear. The vectors can be labelled using an affective lexicon to train a word-level affective classifier. This classifier can be used to expand the original lexicon. BibTex , original paper LabelWordVectors : labels word vectors with an input lexicon in arff format. This filter is useful for training word-level affective classifiers.","title":"Word-level Filters"},{"location":"#distant-supervision-filters","text":"ASA : Annotate-Sample-Average (ASA) is a lexicon-based distant supervision method for training polarity classifiers in Twitter in the absence of labelled data. It takes a collection of unlabelled tweets and a polarity lexicon in arff format and creates synthetic labelled instances. Each labelled instance is created by sampling with replacement a number of tweets containing at least one word from the lexicon with the desired polarity, and averaging the feature vectors of the sampled tweets. BibTex , original paper PTCM : The Partitioned Tweet Centroid Model (PTCM) is an adaption of the TweetCentroidModel for distant supervision. As tweets and words are represented by the same feature vectors, a word-level classifier trained from a polarity lexicon and a corpus of unlabelled tweets can be used for classifying the sentiment of tweets represented by sparse feature vectors. In other words, the labelled word vectors correspond to lexicon-annotated training data for message-level polarity classification. The model includes a simple modification to the tweet centroid model for increasing the number of labelled instances, yielding partitioned tweet centroids . This modification is based on partitioning the tweets associated with each word into smaller disjoint subsets of a fixed size. The method calculates one centroid per partition, which is labelled according to the lexicon. BibTex , original paper LexiconDistantSupervision : This is the most popular distant supervision approach for Twitter sentiment analysis. It takes a collection of unlabelled tweets and a polarity lexicon in arff format of positive and negative tokens. If a word from the lexicon is found, the tweet is labelled with the word's polarity. Tweets with both positive and negative words are discarded. The word used for labelling the tweet can be removed from the content. Emoticons are used as the default lexicon. original paper","title":"Distant Supervision Filters"},{"location":"#tokenizers","text":"TweetNLPTokenizer : a Twitter-specific String tokenizer based on the CMU Tweet NLP tool that can be used with the existing StringWordToVector Weka filter.","title":"Tokenizers"},{"location":"#other-resources","text":"Datasets : The package provides some tweets annotated by affective values in gzipped ARFF format in $WEKA_HOME/packages/AffectiveTweets/data/. The default location for $WEKA_HOME is $HOME/wekafiles. Affective Lexicons : The package provides affective lexicons in ARFF format. These lexicons are located in $WEKA_HOME/packages/AffectiveTweets/lexicons/arff_lexicons/ and can be used with the TweetToInputLexiconFeatureVector filter. Pre-trained Word-Embeddings : The package provides a file with pre-trained word vectors trained with the Word2Vec tool in gzip compressed format. It is a tab separated file with the word in last column located in $WEKA_HOME/packages/AffectiveTweets/resources/w2v.twitter.edinburgh.100d.csv.gz. However, this is a toy example trained from a small collection of tweets. We recommend downloading w2v.twitter.edinburgh10M.400d.csv.gz , which provides embeddings trained from 10 million tweets taken from the Edinburgh corpus . The parameters were calibrated for classifying words into emotions. More info in this paper .","title":"Other Resources"},{"location":"#documentation","text":"The Java documentation is available here .","title":"Documentation"},{"location":"#team","text":"","title":"Team"},{"location":"#main-developer","text":"Felipe Bravo-Marquez","title":"Main Developer"},{"location":"#contributors","text":"Saif Mohammad Eibe Frank Bernhard Pfahringer","title":"Contributors"},{"location":"#contact","text":"Email: fbravo at dcc.uchile.cl If you have questions about Weka please refer to the Weka mailing list .","title":"Contact"},{"location":"benchmark/","text":"On this page we show how to benchmark models created using AffectiveTweets against similar models created using the NLTK sentiment analysis module and Scikit-learn . We represents tweets using word n-grams and lexicon-based features and train logistic regression models on the Twitter Message Polarity Classification dataset from the SemEval 2013 Sentiment Analysis Task . The code for reproducing these experiments can be downloaded from here . AffectiveTweets Scripts The following bash scripts assume that AffectiveTweets and Weka are already installed. Declare the following variables according to your installation paths. export WEKA_HOME=/home/fbravoma/wekafiles/ export WEKA_PATH=/home/fbravoma/weka-3-9-3/ We need to transform the training and testing datasets into Arff format: java -cp $WEKA_HOME/packages/AffectiveTweets/AffectiveTweets.jar:$WEKA_PATH/weka.jar weka.core.converters.SemEvalToArff benchmark/dataset/twitter-train-B.txt benchmark/dataset/twitter-train-B.arff java -cp $WEKA_HOME/packages/AffectiveTweets/AffectiveTweets.jar:$WEKA_PATH/weka.jar weka.core.converters.SemEvalToArff benchmark/dataset/twitter-test-gold-B.tsv benchmark/dataset/twitter-test-gold-B.arff Logistic regression model using word n-grams (n=1,2,3,4). We train a logistic regression model using word n-grams as features with marked negation using the Weka command-line interface: java -Xmx4G -cp $WEKA_PATH/weka.jar weka.Run weka.classifiers.meta.FilteredClassifier -v -o -t benchmark/dataset/twitter-train-B.arff -T benchmark/dataset/twitter-test-gold-B.arff -F \"weka.filters.MultiFilter -F \\\"weka.filters.unsupervised.attribute.TweetToSparseFeatureVector -E 5 -D 3 -I 0 -F -M 3 -R -G 0 -taggerFile $WEKA_HOME/packages/AffectiveTweets/resources/model.20120919 -wordClustFile $WEKA_HOME/packages/AffectiveTweets/resources/50mpaths2.txt.gz -Q 4 -red -stan -stemmer weka.core.stemmers.NullStemmer -stopwords-handler \\\\\\\"weka.core.stopwords.Null \\\\\\\" -I 1 -U -tokenizer \\\\\\\"weka.core.tokenizers.TweetNLPTokenizer \\\\\\\"\\\" -F \\\"weka.filters.unsupervised.attribute.Reorder -R 3-last,2\\\"\" -S 1 -W weka.classifiers.functions.LibLINEAR -- -S 7 -C 1.0 -E 0.001 -B 1.0 -P -L 0.1 -I 1000 LibLinear allows implementing various linear models (e.g., SVMs, logistics regression) by changing the loss function. In this and the following benchmark experiments involving AffectiveTwets and LibLinear we use L2-regularized logistic regression models. Results: Time taken to test model on test data: 1.56 seconds === Error on test data === Correctly Classified Instances 2545 66.7453 % Incorrectly Classified Instances 1268 33.2547 % Kappa statistic 0.4457 Mean absolute error 0.2642 Root mean squared error 0.3945 Relative absolute error 59.454 % Root relative squared error 83.6944 % Total Number of Instances 3813 === Detailed Accuracy By Class === TP Rate FP Rate Precision Recall F-Measure MCC ROC Area PRC Area Class 0.616 0.136 0.760 0.616 0.680 0.501 0.816 0.789 positive 0.829 0.388 0.617 0.829 0.708 0.442 0.799 0.724 neutral 0.361 0.037 0.644 0.361 0.463 0.416 0.851 0.559 negative Weighted Avg. 0.667 0.229 0.681 0.667 0.658 0.462 0.814 0.725 === Confusion Matrix === a b c <-- classified as 968 543 61 | a = positive 221 1360 59 | b = neutral 84 300 217 | c = negative Logistic regression model using word n-grams + Bing Liu's Lexicon We train a logistic regression model word n-grams and features derived from Bing Liu's Lexicon: java -Xmx4G -cp $WEKA_PATH/weka.jar weka.Run weka.classifiers.meta.FilteredClassifier -v -o -t dataset/twitter-train-B.arff -T dataset/twitter-test-gold-B.arff -F \"weka.filters.MultiFilter -F \\\"weka.filters.unsupervised.attribute.TweetToSparseFeatureVector -E 5 -D 3 -I 0 -F -M 3 -R -G 0 -taggerFile $WEKA_HOME/packages/AffectiveTweets/resources/model.20120919 -wordClustFile $WEKA_HOME/packages/AffectiveTweets/resources/50mpaths2.txt.gz -Q 4 -red -stan -stemmer weka.core.stemmers.NullStemmer -stopwords-handler \\\\\\\"weka.core.stopwords.Null \\\\\\\" -I 1 -U -tokenizer \\\\\\\"weka.core.tokenizers.TweetNLPTokenizer \\\\\\\"\\\" -F \\\"weka.filters.unsupervised.attribute.TweetToLexiconFeatureVector -D -red -stan -stemmer weka.core.stemmers.NullStemmer -stopwords-handler \\\\\\\"weka.core.stopwords.Null \\\\\\\" -I 1 -U -tokenizer \\\\\\\"weka.core.tokenizers.TweetNLPTokenizer \\\\\\\"\\\" -F \\\"weka.filters.unsupervised.attribute.Reorder -R 3-last,2\\\"\" -S 1 -W weka.classifiers.functions.LibLINEAR -- -S 7 -C 1.0 -E 0.001 -B 1.0 -P -L 0.1 -I 1000 Results: Time taken to test model on test data: 10.73 seconds === Error on test data === Correctly Classified Instances 2612 68.5025 % Incorrectly Classified Instances 1201 31.4975 % Kappa statistic 0.4779 Mean absolute error 0.2471 Root mean squared error 0.383 Relative absolute error 55.596 % Root relative squared error 81.2491 % Total Number of Instances 3813 === Detailed Accuracy By Class === TP Rate FP Rate Precision Recall F-Measure MCC ROC Area PRC Area Class 0.641 0.143 0.758 0.641 0.695 0.514 0.837 0.807 positive 0.820 0.349 0.640 0.820 0.719 0.469 0.818 0.751 neutral 0.431 0.038 0.680 0.431 0.527 0.477 0.884 0.616 negative Weighted Avg. 0.685 0.215 0.695 0.685 0.679 0.489 0.836 0.753 === Confusion Matrix === a b c <-- classified as 1008 502 62 | a = positive 235 1345 60 | b = neutral 86 256 259 | c = negative Logistic regression model using Bing Liu's Lexicon + SentiStrength We train a logistic regression using features derived from Bing Liu's Lexicon and the SentiStrength method: java -Xmx4G -cp $WEKA_PATH/weka.jar weka.Run weka.classifiers.meta.FilteredClassifier -v -o -t benchmark/dataset/twitter-train-B.arff -T benchmark/dataset/twitter-test-gold-B.arff -F \"weka.filters.MultiFilter -F \\\"weka.filters.unsupervised.attribute.TweetToSentiStrengthFeatureVector -L $WEKA_HOME/packages/AffectiveTweets/lexicons/SentiStrength/english -stan -stemmer weka.core.stemmers.NullStemmer -stopwords-handler \\\\\\\"weka.core.stopwords.Null \\\\\\\" -I 1 -U -tokenizer \\\\\\\"weka.core.tokenizers.TweetNLPTokenizer \\\\\\\"\\\" -F \\\"weka.filters.unsupervised.attribute.TweetToLexiconFeatureVector -D -red -stan -stemmer weka.core.stemmers.NullStemmer -stopwords-handler \\\\\\\"weka.core.stopwords.Null \\\\\\\" -I 1 -U -tokenizer \\\\\\\"weka.core.tokenizers.TweetNLPTokenizer \\\\\\\"\\\" -F \\\"weka.filters.unsupervised.attribute.Reorder -R 3-last,2\\\"\" -S 1 -W weka.classifiers.functions.LibLINEAR -- -S 7 -C 1.0 -E 0.001 -B 1.0 -P -L 0.1 -I 1000 Results: Time taken to test model on test data: 3.32 seconds === Error on test data === Correctly Classified Instances 2457 64.4375 % Incorrectly Classified Instances 1356 35.5625 % Kappa statistic 0.4029 Mean absolute error 0.3198 Root mean squared error 0.4016 Relative absolute error 71.9598 % Root relative squared error 85.1839 % Total Number of Instances 3813 === Detailed Accuracy By Class === TP Rate FP Rate Precision Recall F-Measure MCC ROC Area PRC Area Class 0.622 0.171 0.718 0.622 0.666 0.463 0.794 0.714 positive 0.802 0.399 0.603 0.802 0.688 0.403 0.764 0.667 neutral 0.275 0.033 0.611 0.275 0.379 0.344 0.790 0.483 negative Weighted Avg. 0.644 0.247 0.651 0.644 0.630 0.418 0.781 0.657 === Confusion Matrix === a b c <-- classified as 977 537 58 | a = positive 278 1315 47 | b = neutral 106 330 165 | c = negative Logistic regression model using n-grams + Bing Liu's Lexicon + SentiStrength Now we combine the features from the two previous examples: java -Xmx4G -cp $WEKA_PATH/weka.jar weka.Run weka.classifiers.meta.FilteredClassifier -v -o -t benchmark/dataset/twitter-train-B.arff -T benchmark/dataset/twitter-test-gold-B.arff -F \"weka.filters.MultiFilter -F \\\"weka.filters.unsupervised.attribute.TweetToSparseFeatureVector -E 5 -D 3 -I 0 -F -M 3 -R -G 0 -taggerFile $WEKA_HOME/packages/AffectiveTweets/resources/model.20120919 -wordClustFile $WEKA_HOME/packages/AffectiveTweets/resources/50mpaths2.txt.gz -Q 4 -red -stan -stemmer weka.core.stemmers.NullStemmer -stopwords-handler \\\\\\\"weka.core.stopwords.Null \\\\\\\" -I 1 -U -tokenizer \\\\\\\"weka.core.tokenizers.TweetNLPTokenizer \\\\\\\"\\\" -F \\\"weka.filters.unsupervised.attribute.TweetToSentiStrengthFeatureVector -L $WEKA_HOME/packages/AffectiveTweets/lexicons/SentiStrength/english -stan -stemmer weka.core.stemmers.NullStemmer -stopwords-handler \\\\\\\"weka.core.stopwords.Null \\\\\\\" -I 1 -U -tokenizer \\\\\\\"weka.core.tokenizers.TweetNLPTokenizer \\\\\\\"\\\" -F \\\"weka.filters.unsupervised.attribute.TweetToLexiconFeatureVector -D -red -stan -stemmer weka.core.stemmers.NullStemmer -stopwords-handler \\\\\\\"weka.core.stopwords.Null \\\\\\\" -I 1 -U -tokenizer \\\\\\\"weka.core.tokenizers.TweetNLPTokenizer \\\\\\\"\\\" -F \\\"weka.filters.unsupervised.attribute.Reorder -R 3-last,2\\\"\" -S 1 -W weka.classifiers.functions.LibLINEAR -- -S 7 -C 1.0 -E 0.001 -B 1.0 -P -L 0.1 -I 1000 Results: Time taken to test model on test data: 13.04 seconds === Error on test data === Correctly Classified Instances 2648 69.4466 % Incorrectly Classified Instances 1165 30.5534 % Kappa statistic 0.4949 Mean absolute error 0.2401 Root mean squared error 0.3786 Relative absolute error 54.0243 % Root relative squared error 80.3157 % Total Number of Instances 3813 === Detailed Accuracy By Class === TP Rate FP Rate Precision Recall F-Measure MCC ROC Area PRC Area Class 0.649 0.139 0.766 0.649 0.703 0.527 0.845 0.812 positive 0.823 0.335 0.649 0.823 0.726 0.485 0.826 0.766 neutral 0.463 0.039 0.690 0.463 0.554 0.502 0.896 0.642 negative Weighted Avg. 0.694 0.208 0.704 0.694 0.689 0.505 0.845 0.766 === Confusion Matrix === a b c <-- classified as 1021 485 66 | a = positive 232 1349 59 | b = neutral 80 243 278 | c = negative Logistic regression model model using n-grams + SentiStrength + all lexicons Now we include features from all the lexicons implemented by AffectiveTweets: java -Xmx4G -cp $WEKA_PATH/weka.jar weka.Run weka.classifiers.meta.FilteredClassifier -v -o -t benchmark/dataset/twitter-train-B.arff -T benchmark/dataset/twitter-test-gold-B.arff -F \"weka.filters.MultiFilter -F \\\"weka.filters.unsupervised.attribute.TweetToSparseFeatureVector -E 5 -D 3 -I 0 -F -M 3 -R -G 0 -taggerFile $WEKA_HOME/packages/AffectiveTweets/resources/model.20120919 -wordClustFile $WEKA_HOME/packages/AffectiveTweets/resources/50mpaths2.txt.gz -Q 4 -red -stan -stemmer weka.core.stemmers.NullStemmer -stopwords-handler \\\\\\\"weka.core.stopwords.Null \\\\\\\" -I 1 -U -tokenizer \\\\\\\"weka.core.tokenizers.TweetNLPTokenizer \\\\\\\"\\\" -F \\\"weka.filters.unsupervised.attribute.TweetToSentiStrengthFeatureVector -L $WEKA_HOME/packages/AffectiveTweets/lexicons/SentiStrength/english -stan -stemmer weka.core.stemmers.NullStemmer -stopwords-handler \\\\\\\"weka.core.stopwords.Null \\\\\\\" -I 1 -U -tokenizer \\\\\\\"weka.core.tokenizers.TweetNLPTokenizer \\\\\\\"\\\" -F \\\"weka.filters.unsupervised.attribute.TweetToLexiconFeatureVector -F -D -R -A -N -P -J -H -Q -red -stan -stemmer weka.core.stemmers.NullStemmer -stopwords-handler \\\\\\\"weka.core.stopwords.Null \\\\\\\" -I 1 -U -tokenizer \\\\\\\"weka.core.tokenizers.TweetNLPTokenizer \\\\\\\"\\\" -F \\\"weka.filters.unsupervised.attribute.Reorder -R 3-last,2\\\"\" -S 1 -W weka.classifiers.functions.LibLINEAR -- -S 7 -C 1.0 -E 0.001 -B 1.0 -P -L 0.1 -I 1000 Results: Time taken to test model on test data: 13.29 seconds === Error on test data === Correctly Classified Instances 2706 70.9677 % Incorrectly Classified Instances 1107 29.0323 % Kappa statistic 0.5215 Mean absolute error 0.2289 Root mean squared error 0.372 Relative absolute error 51.5039 % Root relative squared error 78.9079 % Total Number of Instances 3813 === Detailed Accuracy By Class === TP Rate FP Rate Precision Recall F-Measure MCC ROC Area PRC Area Class 0.658 0.131 0.779 0.658 0.713 0.544 0.854 0.823 positive 0.831 0.319 0.663 0.831 0.738 0.509 0.835 0.772 neutral 0.514 0.037 0.720 0.514 0.600 0.550 0.913 0.687 negative Weighted Avg. 0.710 0.197 0.720 0.710 0.706 0.530 0.855 0.779 === Confusion Matrix === a b c <-- classified as 1034 471 67 | a = positive 224 1363 53 | b = neutral 70 222 309 | c = negative NLTK + SciKit-learn Scripts Now we will build similar models using python 3.6 . First we need to import the following libraries. import pandas as pd from nltk.tokenize import TweetTokenizer from nltk.sentiment import SentimentIntensityAnalyzer from nltk.sentiment.util import mark_negation from nltk.corpus import opinion_lexicon from sklearn.feature_extraction.text import CountVectorizer from sklearn.linear_model import LogisticRegression from sklearn.pipeline import Pipeline, FeatureUnion from sklearn.base import BaseEstimator, TransformerMixin from sklearn.metrics import confusion_matrix, cohen_kappa_score import numpy as np Make sure to install all of them using pip or conda . Next, load training and testing datasets as pandas dataframes: # load training and testing datasets as a pandas dataframe train_data = pd.read_csv(\"dataset/twitter-train-B.txt\", header=None, delimiter=\"\\t\",usecols=(2,3), names=(\"sent\",\"tweet\")) test_data = pd.read_csv(\"dataset/twitter-test-gold-B.tsv\", header=None, delimiter=\"\\t\",usecols=(2,3), names=(\"sent\",\"tweet\")) # replace objective-OR-neutral and objective to neutral train_data.sent = train_data.sent.replace(['objective-OR-neutral','objective'],['neutral','neutral']) # use a Twitter-specific tokenizer tokenizer = TweetTokenizer(preserve_case=False, reduce_len=True) Logistic regression model using word n-grams (n=1,2,3,4). We replicate the same model we created previously using AffectiveTweets. N-grams are extracted using CountVectorizer from Scikit-learn. N-grams inside a negation word are marked. vectorizer = CountVectorizer(tokenizer = tokenizer.tokenize, preprocessor = mark_negation, ngram_range=(1,4)) log_mod = LogisticRegression(solver='liblinear',multi_class='ovr') text_clf = Pipeline([('vect', vectorizer), ('clf', log_mod)]) text_clf.fit(train_data.tweet, train_data.sent) predicted = text_clf.predict(test_data.tweet) conf = confusion_matrix(test_data.sent, predicted) kappa = cohen_kappa_score(test_data.sent, predicted) class_rep = classification_report(test_data.sent, predicted) print('Confusion Matrix for Logistic Regression + ngram features:') print(conf) print('Classification Report') print(class_rep) print('kappa:'+str(kappa)) Results: Confusion Matrix for Logistic Regression + ngram features: [[ 172 335 94] [ 31 1433 176] [ 48 620 904]] Classification Report precision recall f1-score support negative 0.69 0.29 0.40 601 neutral 0.60 0.87 0.71 1640 positive 0.77 0.58 0.66 1572 avg / total 0.68 0.66 0.64 3813 kappa:0.4236034809946826 Logistic regression model using word n-grams + Bing Liu's Lexicon We replicate now the second model created using AffectiveTweets: a logistic regression trained on word n-grams and features calculated from Bing Liu's Lexicon. First, we need to make sure that the required NLTK resources are installed: import nltk nltk.download('opinion_lexicon') We extend Scikit-learn classes BaseEstimator and TransformerMixin to implement a feature extractor that uses Bing Liu's lexicon: class LiuFeatureExtractor(BaseEstimator, TransformerMixin): \"\"\"Takes in a corpus of tweets and calculates features using Bing Liu's lexicon\"\"\" def __init__(self, tokenizer): self.tokenizer = tokenizer self.pos_set = set(opinion_lexicon.positive()) self.neg_set = set(opinion_lexicon.negative()) def liu_score(self,sentence): \"\"\"Calculates the number of positive and negative words in the sentence using Bing Liu's Lexicon\"\"\" tokenized_sent = self.tokenizer.tokenize(sentence) pos_words = 0 neg_words = 0 for word in tokenized_sent: if word in self.pos_set: pos_words += 1 elif word in self.neg_set: neg_words += 1 return [pos_words,neg_words] def transform(self, X, y=None): \"\"\"Applies liu_score and vader_score on a data.frame containing tweets \"\"\" values = [] for tweet in X: values.append(self.liu_score(tweet)) return(np.array(values)) def fit(self, X, y=None): \"\"\"This function must return `self` unless we expect the transform function to perform a different action on training and testing partitions (e.g., when we calculate unigram features, the dictionary is only extracted from the first batch)\"\"\" return self We can combine word n-gram features and features derived from Bing Liu's lexicon using the class FeatureUnion from Scikit-learn: liu_feat = LiuFeatureExtractor(tokenizer) vectorizer = CountVectorizer(tokenizer = tokenizer.tokenize, preprocessor = mark_negation, ngram_range=(1,4)) log_mod = LogisticRegression(solver='liblinear',multi_class='ovr') liu_ngram_clf = Pipeline([ ('feats', FeatureUnion([ ('ngram', vectorizer), ('liu',liu_feat) ])), ('clf', log_mod)]) liu_ngram_clf.fit(train_data.tweet, train_data.sent) pred_liu_ngram = liu_ngram_clf.predict(test_data.tweet) conf_liu_ngram = confusion_matrix(test_data.sent, pred_liu_ngram) kappa_liu_ngram = cohen_kappa_score(test_data.sent, pred_liu_ngram) class_rep_liu_ngram = classification_report(test_data.sent, pred_liu_ngram) print('Confusion Matrix for Logistic Regression + ngrams + features from Bing Liu\\'s Lexicon') print(conf_liu_ngram) print('Classification Report') print(class_rep_liu_ngram) print('kappa:'+str(kappa_liu_ngram)) Results: Confusion Matrix for Logistic Regression + ngrams + features from Bing Liu's Lexicon [[ 236 290 75] [ 44 1395 201] [ 59 529 984]] Classification Report precision recall f1-score support negative 0.70 0.39 0.50 601 neutral 0.63 0.85 0.72 1640 positive 0.78 0.63 0.69 1572 avg / total 0.70 0.69 0.68 3813 kappa:0.4763629485702495 Logistic regression model using Bing Liu's Lexicon + Vader Unfortunately, SentiStrength is not implemented in the NLTK sentiment module. However, NLTK implements Vader , which is another popular lexicon-based sentiment analysis method. We implement a logistic regression using features derived from Bing Liu's lexicon and Vader. First, we need to make sure that the required NLTK resources are installed: import nltk nltk.download('opinion_lexicon') nltk.download('vader_lexicon') We implement another feature extractor that calculates features using Vader: class VaderFeatureExtractor(BaseEstimator, TransformerMixin): \"\"\"Takes in a corpus of tweets and calculates features using the Vader method\"\"\" def __init__(self, tokenizer): self.tokenizer = tokenizer self.sid = SentimentIntensityAnalyzer() def vader_score(self,sentence): \"\"\" Calculates sentiment scores for a sentence using the Vader method \"\"\" pol_scores = self.sid.polarity_scores(sentence) return(list(pol_scores.values())) def transform(self, X, y=None): \"\"\"Applies vader_score on a data.frame containing tweets \"\"\" values = [] for tweet in X: values.append(self.vader_score(tweet)) return(np.array(values)) def fit(self, X, y=None): \"\"\"Returns `self` unless something different happens in train and test\"\"\" return self vader_feat = VaderFeatureExtractor(tokenizer) liu_feat = LiuFeatureExtractor(tokenizer) log_mod = LogisticRegression(solver='liblinear',multi_class='ovr') vader_liu_clf = Pipeline([ ('feats', FeatureUnion([ ('vader', vader_feat), ('liu',liu_feat) ])), ('clf', log_mod)]) vader_liu_clf.fit(train_data.tweet, train_data.sent) pred_vader_liu = vader_liu_clf.predict(test_data.tweet) conf_vader_liu = confusion_matrix(test_data.sent, pred_vader_liu) kappa_vader_liu = cohen_kappa_score(test_data.sent, pred_vader_liu) class_rep_vader_liu = classification_report(test_data.sent, pred_vader_liu) print('Confusion Matrix for Logistic Regression + Vader + features from Bing Liu\\'s Lexicon') print(conf_vader_liu) print('Classification Report') print(class_rep_vader_liu) print('kappa:'+str(kappa_vader_liu)) Results: Confusion Matrix for Logistic Regression + Vader + features from Bing Liu's Lexicon [[ 169 323 109] [ 51 1275 314] [ 58 491 1023]] Classification Report precision recall f1-score support negative 0.61 0.28 0.38 601 neutral 0.61 0.78 0.68 1640 positive 0.71 0.65 0.68 1572 avg / total 0.65 0.65 0.63 3813 kappa:0.408231856331834 Logistic regression model using n-grams + Bing Liu's Lexicon + Vader We now combine the feature space of all the previous examples: ngram_lex_clf = Pipeline([ ('feats', FeatureUnion([ ('ngram', vectorizer), ('vader',vader_feat),('liu',liu_feat) ])), ('clf', log_mod)]) ngram_lex_clf.fit(train_data.tweet, train_data.sent) pred_ngram_lex = ngram_lex_clf.predict(test_data.tweet) conf_ngram_lex = confusion_matrix(test_data.sent, pred_ngram_lex) kappa_ngram_lex = cohen_kappa_score(test_data.sent, pred_ngram_lex) class_rep = classification_report(test_data.sent, pred_ngram_lex) print('Confusion Matrix for Logistic Regression + ngrams + features from Bing Liu\\'s Lexicon and the Vader method') print(conf_ngram_lex) print('Classification Report') print(class_rep) print('kappa:'+str(kappa_ngram_lex)) Results: Confusion Matrix for Logistic Regression + ngrams + features from Bing Liu's Lexicon and the Vader method [[ 268 261 72] [ 45 1387 208] [ 56 493 1023]] Classification Report precision recall f1-score support negative 0.73 0.45 0.55 601 neutral 0.65 0.85 0.73 1640 positive 0.79 0.65 0.71 1572 avg / total 0.72 0.70 0.70 3813 kappa:0.5058311344923361 Summary of Results A table summarising all the experiments from above is shown as follows: Features Implementation Kappa Score F1 Score Time (Seconds) Word n-grams Scikitlearn + NLTK 0.42 0.64 30.7 Word n-grams AffectiveTweets 0.45 0.66 13.0 Word n-grams + Liu Lexicon Scikitlearn + NLTK 0.48 0.68 13.4 Word n-grams + Liu Lexicon AffectiveTweets 0.48 0.68 27.4 Liu Lexicon + Vader Scikitlearn + NLTK 0.41 0.63 8.9 Liu Lexicon + SentiStrength AffectiveTweets 0.40 0.63 31.9 Word n-grams + Liu Lexicon + Vader Scikitlearn + NLTK 0.51 0.70 16 Word n-grams + Liu Lexicon + SentiStrength AffectiveTweets 0.49 0.69 68.5 Word n-grams + All lexicons + SentiStrength AffectiveTweets 0.52 0.71 74.6 The execution time is averaged over 10 repetitions of each model. Bear in mind that there are only two models (word n-grams and word n-grams+Liu Lexicon) that can be directly compared in both implementations (AffectiveTweets and Scikitlearn+NLTK) as they use the same features and the same learning schemes. Other examples such as Liu Lexicon+Vader and Liu Lexicon+SentiStregnth show how similar models can be implemented using two different tools. The experiments were performed on an Intel(R) Core(TM) i7-2600 CPU @ 3.40GHz with 16 GB of RAM using Ubuntu 16.04.4 LTS. AfftectiveTweets models were run using Weka 3.9.3 and Java 8 (Oracle version). Scikitlearn+NLTK models were run using Python 3.6.4 (Anaconda version), Scikitlearn 0.20.3 and NLTK 3.4.1.","title":"Benchmark"},{"location":"benchmark/#affectivetweets-scripts","text":"The following bash scripts assume that AffectiveTweets and Weka are already installed. Declare the following variables according to your installation paths. export WEKA_HOME=/home/fbravoma/wekafiles/ export WEKA_PATH=/home/fbravoma/weka-3-9-3/ We need to transform the training and testing datasets into Arff format: java -cp $WEKA_HOME/packages/AffectiveTweets/AffectiveTweets.jar:$WEKA_PATH/weka.jar weka.core.converters.SemEvalToArff benchmark/dataset/twitter-train-B.txt benchmark/dataset/twitter-train-B.arff java -cp $WEKA_HOME/packages/AffectiveTweets/AffectiveTweets.jar:$WEKA_PATH/weka.jar weka.core.converters.SemEvalToArff benchmark/dataset/twitter-test-gold-B.tsv benchmark/dataset/twitter-test-gold-B.arff","title":"AffectiveTweets Scripts"},{"location":"benchmark/#logistic-regression-model-using-word-n-grams-n1234","text":"We train a logistic regression model using word n-grams as features with marked negation using the Weka command-line interface: java -Xmx4G -cp $WEKA_PATH/weka.jar weka.Run weka.classifiers.meta.FilteredClassifier -v -o -t benchmark/dataset/twitter-train-B.arff -T benchmark/dataset/twitter-test-gold-B.arff -F \"weka.filters.MultiFilter -F \\\"weka.filters.unsupervised.attribute.TweetToSparseFeatureVector -E 5 -D 3 -I 0 -F -M 3 -R -G 0 -taggerFile $WEKA_HOME/packages/AffectiveTweets/resources/model.20120919 -wordClustFile $WEKA_HOME/packages/AffectiveTweets/resources/50mpaths2.txt.gz -Q 4 -red -stan -stemmer weka.core.stemmers.NullStemmer -stopwords-handler \\\\\\\"weka.core.stopwords.Null \\\\\\\" -I 1 -U -tokenizer \\\\\\\"weka.core.tokenizers.TweetNLPTokenizer \\\\\\\"\\\" -F \\\"weka.filters.unsupervised.attribute.Reorder -R 3-last,2\\\"\" -S 1 -W weka.classifiers.functions.LibLINEAR -- -S 7 -C 1.0 -E 0.001 -B 1.0 -P -L 0.1 -I 1000 LibLinear allows implementing various linear models (e.g., SVMs, logistics regression) by changing the loss function. In this and the following benchmark experiments involving AffectiveTwets and LibLinear we use L2-regularized logistic regression models. Results: Time taken to test model on test data: 1.56 seconds === Error on test data === Correctly Classified Instances 2545 66.7453 % Incorrectly Classified Instances 1268 33.2547 % Kappa statistic 0.4457 Mean absolute error 0.2642 Root mean squared error 0.3945 Relative absolute error 59.454 % Root relative squared error 83.6944 % Total Number of Instances 3813 === Detailed Accuracy By Class === TP Rate FP Rate Precision Recall F-Measure MCC ROC Area PRC Area Class 0.616 0.136 0.760 0.616 0.680 0.501 0.816 0.789 positive 0.829 0.388 0.617 0.829 0.708 0.442 0.799 0.724 neutral 0.361 0.037 0.644 0.361 0.463 0.416 0.851 0.559 negative Weighted Avg. 0.667 0.229 0.681 0.667 0.658 0.462 0.814 0.725 === Confusion Matrix === a b c <-- classified as 968 543 61 | a = positive 221 1360 59 | b = neutral 84 300 217 | c = negative","title":"Logistic regression model using word n-grams (n=1,2,3,4)."},{"location":"benchmark/#logistic-regression-model-using-word-n-grams-bing-lius-lexicon","text":"We train a logistic regression model word n-grams and features derived from Bing Liu's Lexicon: java -Xmx4G -cp $WEKA_PATH/weka.jar weka.Run weka.classifiers.meta.FilteredClassifier -v -o -t dataset/twitter-train-B.arff -T dataset/twitter-test-gold-B.arff -F \"weka.filters.MultiFilter -F \\\"weka.filters.unsupervised.attribute.TweetToSparseFeatureVector -E 5 -D 3 -I 0 -F -M 3 -R -G 0 -taggerFile $WEKA_HOME/packages/AffectiveTweets/resources/model.20120919 -wordClustFile $WEKA_HOME/packages/AffectiveTweets/resources/50mpaths2.txt.gz -Q 4 -red -stan -stemmer weka.core.stemmers.NullStemmer -stopwords-handler \\\\\\\"weka.core.stopwords.Null \\\\\\\" -I 1 -U -tokenizer \\\\\\\"weka.core.tokenizers.TweetNLPTokenizer \\\\\\\"\\\" -F \\\"weka.filters.unsupervised.attribute.TweetToLexiconFeatureVector -D -red -stan -stemmer weka.core.stemmers.NullStemmer -stopwords-handler \\\\\\\"weka.core.stopwords.Null \\\\\\\" -I 1 -U -tokenizer \\\\\\\"weka.core.tokenizers.TweetNLPTokenizer \\\\\\\"\\\" -F \\\"weka.filters.unsupervised.attribute.Reorder -R 3-last,2\\\"\" -S 1 -W weka.classifiers.functions.LibLINEAR -- -S 7 -C 1.0 -E 0.001 -B 1.0 -P -L 0.1 -I 1000 Results: Time taken to test model on test data: 10.73 seconds === Error on test data === Correctly Classified Instances 2612 68.5025 % Incorrectly Classified Instances 1201 31.4975 % Kappa statistic 0.4779 Mean absolute error 0.2471 Root mean squared error 0.383 Relative absolute error 55.596 % Root relative squared error 81.2491 % Total Number of Instances 3813 === Detailed Accuracy By Class === TP Rate FP Rate Precision Recall F-Measure MCC ROC Area PRC Area Class 0.641 0.143 0.758 0.641 0.695 0.514 0.837 0.807 positive 0.820 0.349 0.640 0.820 0.719 0.469 0.818 0.751 neutral 0.431 0.038 0.680 0.431 0.527 0.477 0.884 0.616 negative Weighted Avg. 0.685 0.215 0.695 0.685 0.679 0.489 0.836 0.753 === Confusion Matrix === a b c <-- classified as 1008 502 62 | a = positive 235 1345 60 | b = neutral 86 256 259 | c = negative","title":"Logistic regression model using word n-grams + Bing Liu's Lexicon"},{"location":"benchmark/#logistic-regression-model-using-bing-lius-lexicon-sentistrength","text":"We train a logistic regression using features derived from Bing Liu's Lexicon and the SentiStrength method: java -Xmx4G -cp $WEKA_PATH/weka.jar weka.Run weka.classifiers.meta.FilteredClassifier -v -o -t benchmark/dataset/twitter-train-B.arff -T benchmark/dataset/twitter-test-gold-B.arff -F \"weka.filters.MultiFilter -F \\\"weka.filters.unsupervised.attribute.TweetToSentiStrengthFeatureVector -L $WEKA_HOME/packages/AffectiveTweets/lexicons/SentiStrength/english -stan -stemmer weka.core.stemmers.NullStemmer -stopwords-handler \\\\\\\"weka.core.stopwords.Null \\\\\\\" -I 1 -U -tokenizer \\\\\\\"weka.core.tokenizers.TweetNLPTokenizer \\\\\\\"\\\" -F \\\"weka.filters.unsupervised.attribute.TweetToLexiconFeatureVector -D -red -stan -stemmer weka.core.stemmers.NullStemmer -stopwords-handler \\\\\\\"weka.core.stopwords.Null \\\\\\\" -I 1 -U -tokenizer \\\\\\\"weka.core.tokenizers.TweetNLPTokenizer \\\\\\\"\\\" -F \\\"weka.filters.unsupervised.attribute.Reorder -R 3-last,2\\\"\" -S 1 -W weka.classifiers.functions.LibLINEAR -- -S 7 -C 1.0 -E 0.001 -B 1.0 -P -L 0.1 -I 1000 Results: Time taken to test model on test data: 3.32 seconds === Error on test data === Correctly Classified Instances 2457 64.4375 % Incorrectly Classified Instances 1356 35.5625 % Kappa statistic 0.4029 Mean absolute error 0.3198 Root mean squared error 0.4016 Relative absolute error 71.9598 % Root relative squared error 85.1839 % Total Number of Instances 3813 === Detailed Accuracy By Class === TP Rate FP Rate Precision Recall F-Measure MCC ROC Area PRC Area Class 0.622 0.171 0.718 0.622 0.666 0.463 0.794 0.714 positive 0.802 0.399 0.603 0.802 0.688 0.403 0.764 0.667 neutral 0.275 0.033 0.611 0.275 0.379 0.344 0.790 0.483 negative Weighted Avg. 0.644 0.247 0.651 0.644 0.630 0.418 0.781 0.657 === Confusion Matrix === a b c <-- classified as 977 537 58 | a = positive 278 1315 47 | b = neutral 106 330 165 | c = negative","title":"Logistic regression model using Bing Liu's Lexicon + SentiStrength"},{"location":"benchmark/#logistic-regression-model-using-n-grams-bing-lius-lexicon-sentistrength","text":"Now we combine the features from the two previous examples: java -Xmx4G -cp $WEKA_PATH/weka.jar weka.Run weka.classifiers.meta.FilteredClassifier -v -o -t benchmark/dataset/twitter-train-B.arff -T benchmark/dataset/twitter-test-gold-B.arff -F \"weka.filters.MultiFilter -F \\\"weka.filters.unsupervised.attribute.TweetToSparseFeatureVector -E 5 -D 3 -I 0 -F -M 3 -R -G 0 -taggerFile $WEKA_HOME/packages/AffectiveTweets/resources/model.20120919 -wordClustFile $WEKA_HOME/packages/AffectiveTweets/resources/50mpaths2.txt.gz -Q 4 -red -stan -stemmer weka.core.stemmers.NullStemmer -stopwords-handler \\\\\\\"weka.core.stopwords.Null \\\\\\\" -I 1 -U -tokenizer \\\\\\\"weka.core.tokenizers.TweetNLPTokenizer \\\\\\\"\\\" -F \\\"weka.filters.unsupervised.attribute.TweetToSentiStrengthFeatureVector -L $WEKA_HOME/packages/AffectiveTweets/lexicons/SentiStrength/english -stan -stemmer weka.core.stemmers.NullStemmer -stopwords-handler \\\\\\\"weka.core.stopwords.Null \\\\\\\" -I 1 -U -tokenizer \\\\\\\"weka.core.tokenizers.TweetNLPTokenizer \\\\\\\"\\\" -F \\\"weka.filters.unsupervised.attribute.TweetToLexiconFeatureVector -D -red -stan -stemmer weka.core.stemmers.NullStemmer -stopwords-handler \\\\\\\"weka.core.stopwords.Null \\\\\\\" -I 1 -U -tokenizer \\\\\\\"weka.core.tokenizers.TweetNLPTokenizer \\\\\\\"\\\" -F \\\"weka.filters.unsupervised.attribute.Reorder -R 3-last,2\\\"\" -S 1 -W weka.classifiers.functions.LibLINEAR -- -S 7 -C 1.0 -E 0.001 -B 1.0 -P -L 0.1 -I 1000 Results: Time taken to test model on test data: 13.04 seconds === Error on test data === Correctly Classified Instances 2648 69.4466 % Incorrectly Classified Instances 1165 30.5534 % Kappa statistic 0.4949 Mean absolute error 0.2401 Root mean squared error 0.3786 Relative absolute error 54.0243 % Root relative squared error 80.3157 % Total Number of Instances 3813 === Detailed Accuracy By Class === TP Rate FP Rate Precision Recall F-Measure MCC ROC Area PRC Area Class 0.649 0.139 0.766 0.649 0.703 0.527 0.845 0.812 positive 0.823 0.335 0.649 0.823 0.726 0.485 0.826 0.766 neutral 0.463 0.039 0.690 0.463 0.554 0.502 0.896 0.642 negative Weighted Avg. 0.694 0.208 0.704 0.694 0.689 0.505 0.845 0.766 === Confusion Matrix === a b c <-- classified as 1021 485 66 | a = positive 232 1349 59 | b = neutral 80 243 278 | c = negative","title":"Logistic regression model using n-grams + Bing Liu's Lexicon + SentiStrength"},{"location":"benchmark/#logistic-regression-model-model-using-n-grams-sentistrength-all-lexicons","text":"Now we include features from all the lexicons implemented by AffectiveTweets: java -Xmx4G -cp $WEKA_PATH/weka.jar weka.Run weka.classifiers.meta.FilteredClassifier -v -o -t benchmark/dataset/twitter-train-B.arff -T benchmark/dataset/twitter-test-gold-B.arff -F \"weka.filters.MultiFilter -F \\\"weka.filters.unsupervised.attribute.TweetToSparseFeatureVector -E 5 -D 3 -I 0 -F -M 3 -R -G 0 -taggerFile $WEKA_HOME/packages/AffectiveTweets/resources/model.20120919 -wordClustFile $WEKA_HOME/packages/AffectiveTweets/resources/50mpaths2.txt.gz -Q 4 -red -stan -stemmer weka.core.stemmers.NullStemmer -stopwords-handler \\\\\\\"weka.core.stopwords.Null \\\\\\\" -I 1 -U -tokenizer \\\\\\\"weka.core.tokenizers.TweetNLPTokenizer \\\\\\\"\\\" -F \\\"weka.filters.unsupervised.attribute.TweetToSentiStrengthFeatureVector -L $WEKA_HOME/packages/AffectiveTweets/lexicons/SentiStrength/english -stan -stemmer weka.core.stemmers.NullStemmer -stopwords-handler \\\\\\\"weka.core.stopwords.Null \\\\\\\" -I 1 -U -tokenizer \\\\\\\"weka.core.tokenizers.TweetNLPTokenizer \\\\\\\"\\\" -F \\\"weka.filters.unsupervised.attribute.TweetToLexiconFeatureVector -F -D -R -A -N -P -J -H -Q -red -stan -stemmer weka.core.stemmers.NullStemmer -stopwords-handler \\\\\\\"weka.core.stopwords.Null \\\\\\\" -I 1 -U -tokenizer \\\\\\\"weka.core.tokenizers.TweetNLPTokenizer \\\\\\\"\\\" -F \\\"weka.filters.unsupervised.attribute.Reorder -R 3-last,2\\\"\" -S 1 -W weka.classifiers.functions.LibLINEAR -- -S 7 -C 1.0 -E 0.001 -B 1.0 -P -L 0.1 -I 1000 Results: Time taken to test model on test data: 13.29 seconds === Error on test data === Correctly Classified Instances 2706 70.9677 % Incorrectly Classified Instances 1107 29.0323 % Kappa statistic 0.5215 Mean absolute error 0.2289 Root mean squared error 0.372 Relative absolute error 51.5039 % Root relative squared error 78.9079 % Total Number of Instances 3813 === Detailed Accuracy By Class === TP Rate FP Rate Precision Recall F-Measure MCC ROC Area PRC Area Class 0.658 0.131 0.779 0.658 0.713 0.544 0.854 0.823 positive 0.831 0.319 0.663 0.831 0.738 0.509 0.835 0.772 neutral 0.514 0.037 0.720 0.514 0.600 0.550 0.913 0.687 negative Weighted Avg. 0.710 0.197 0.720 0.710 0.706 0.530 0.855 0.779 === Confusion Matrix === a b c <-- classified as 1034 471 67 | a = positive 224 1363 53 | b = neutral 70 222 309 | c = negative","title":"Logistic regression model model using n-grams + SentiStrength + all lexicons"},{"location":"benchmark/#nltk-scikit-learn-scripts","text":"Now we will build similar models using python 3.6 . First we need to import the following libraries. import pandas as pd from nltk.tokenize import TweetTokenizer from nltk.sentiment import SentimentIntensityAnalyzer from nltk.sentiment.util import mark_negation from nltk.corpus import opinion_lexicon from sklearn.feature_extraction.text import CountVectorizer from sklearn.linear_model import LogisticRegression from sklearn.pipeline import Pipeline, FeatureUnion from sklearn.base import BaseEstimator, TransformerMixin from sklearn.metrics import confusion_matrix, cohen_kappa_score import numpy as np Make sure to install all of them using pip or conda . Next, load training and testing datasets as pandas dataframes: # load training and testing datasets as a pandas dataframe train_data = pd.read_csv(\"dataset/twitter-train-B.txt\", header=None, delimiter=\"\\t\",usecols=(2,3), names=(\"sent\",\"tweet\")) test_data = pd.read_csv(\"dataset/twitter-test-gold-B.tsv\", header=None, delimiter=\"\\t\",usecols=(2,3), names=(\"sent\",\"tweet\")) # replace objective-OR-neutral and objective to neutral train_data.sent = train_data.sent.replace(['objective-OR-neutral','objective'],['neutral','neutral']) # use a Twitter-specific tokenizer tokenizer = TweetTokenizer(preserve_case=False, reduce_len=True)","title":"NLTK + SciKit-learn Scripts"},{"location":"benchmark/#logistic-regression-model-using-word-n-grams-n1234_1","text":"We replicate the same model we created previously using AffectiveTweets. N-grams are extracted using CountVectorizer from Scikit-learn. N-grams inside a negation word are marked. vectorizer = CountVectorizer(tokenizer = tokenizer.tokenize, preprocessor = mark_negation, ngram_range=(1,4)) log_mod = LogisticRegression(solver='liblinear',multi_class='ovr') text_clf = Pipeline([('vect', vectorizer), ('clf', log_mod)]) text_clf.fit(train_data.tweet, train_data.sent) predicted = text_clf.predict(test_data.tweet) conf = confusion_matrix(test_data.sent, predicted) kappa = cohen_kappa_score(test_data.sent, predicted) class_rep = classification_report(test_data.sent, predicted) print('Confusion Matrix for Logistic Regression + ngram features:') print(conf) print('Classification Report') print(class_rep) print('kappa:'+str(kappa)) Results: Confusion Matrix for Logistic Regression + ngram features: [[ 172 335 94] [ 31 1433 176] [ 48 620 904]] Classification Report precision recall f1-score support negative 0.69 0.29 0.40 601 neutral 0.60 0.87 0.71 1640 positive 0.77 0.58 0.66 1572 avg / total 0.68 0.66 0.64 3813 kappa:0.4236034809946826","title":"Logistic regression model using word n-grams (n=1,2,3,4)."},{"location":"benchmark/#logistic-regression-model-using-word-n-grams-bing-lius-lexicon_1","text":"We replicate now the second model created using AffectiveTweets: a logistic regression trained on word n-grams and features calculated from Bing Liu's Lexicon. First, we need to make sure that the required NLTK resources are installed: import nltk nltk.download('opinion_lexicon') We extend Scikit-learn classes BaseEstimator and TransformerMixin to implement a feature extractor that uses Bing Liu's lexicon: class LiuFeatureExtractor(BaseEstimator, TransformerMixin): \"\"\"Takes in a corpus of tweets and calculates features using Bing Liu's lexicon\"\"\" def __init__(self, tokenizer): self.tokenizer = tokenizer self.pos_set = set(opinion_lexicon.positive()) self.neg_set = set(opinion_lexicon.negative()) def liu_score(self,sentence): \"\"\"Calculates the number of positive and negative words in the sentence using Bing Liu's Lexicon\"\"\" tokenized_sent = self.tokenizer.tokenize(sentence) pos_words = 0 neg_words = 0 for word in tokenized_sent: if word in self.pos_set: pos_words += 1 elif word in self.neg_set: neg_words += 1 return [pos_words,neg_words] def transform(self, X, y=None): \"\"\"Applies liu_score and vader_score on a data.frame containing tweets \"\"\" values = [] for tweet in X: values.append(self.liu_score(tweet)) return(np.array(values)) def fit(self, X, y=None): \"\"\"This function must return `self` unless we expect the transform function to perform a different action on training and testing partitions (e.g., when we calculate unigram features, the dictionary is only extracted from the first batch)\"\"\" return self We can combine word n-gram features and features derived from Bing Liu's lexicon using the class FeatureUnion from Scikit-learn: liu_feat = LiuFeatureExtractor(tokenizer) vectorizer = CountVectorizer(tokenizer = tokenizer.tokenize, preprocessor = mark_negation, ngram_range=(1,4)) log_mod = LogisticRegression(solver='liblinear',multi_class='ovr') liu_ngram_clf = Pipeline([ ('feats', FeatureUnion([ ('ngram', vectorizer), ('liu',liu_feat) ])), ('clf', log_mod)]) liu_ngram_clf.fit(train_data.tweet, train_data.sent) pred_liu_ngram = liu_ngram_clf.predict(test_data.tweet) conf_liu_ngram = confusion_matrix(test_data.sent, pred_liu_ngram) kappa_liu_ngram = cohen_kappa_score(test_data.sent, pred_liu_ngram) class_rep_liu_ngram = classification_report(test_data.sent, pred_liu_ngram) print('Confusion Matrix for Logistic Regression + ngrams + features from Bing Liu\\'s Lexicon') print(conf_liu_ngram) print('Classification Report') print(class_rep_liu_ngram) print('kappa:'+str(kappa_liu_ngram)) Results: Confusion Matrix for Logistic Regression + ngrams + features from Bing Liu's Lexicon [[ 236 290 75] [ 44 1395 201] [ 59 529 984]] Classification Report precision recall f1-score support negative 0.70 0.39 0.50 601 neutral 0.63 0.85 0.72 1640 positive 0.78 0.63 0.69 1572 avg / total 0.70 0.69 0.68 3813 kappa:0.4763629485702495","title":"Logistic regression model using word n-grams + Bing Liu's Lexicon"},{"location":"benchmark/#logistic-regression-model-using-bing-lius-lexicon-vader","text":"Unfortunately, SentiStrength is not implemented in the NLTK sentiment module. However, NLTK implements Vader , which is another popular lexicon-based sentiment analysis method. We implement a logistic regression using features derived from Bing Liu's lexicon and Vader. First, we need to make sure that the required NLTK resources are installed: import nltk nltk.download('opinion_lexicon') nltk.download('vader_lexicon') We implement another feature extractor that calculates features using Vader: class VaderFeatureExtractor(BaseEstimator, TransformerMixin): \"\"\"Takes in a corpus of tweets and calculates features using the Vader method\"\"\" def __init__(self, tokenizer): self.tokenizer = tokenizer self.sid = SentimentIntensityAnalyzer() def vader_score(self,sentence): \"\"\" Calculates sentiment scores for a sentence using the Vader method \"\"\" pol_scores = self.sid.polarity_scores(sentence) return(list(pol_scores.values())) def transform(self, X, y=None): \"\"\"Applies vader_score on a data.frame containing tweets \"\"\" values = [] for tweet in X: values.append(self.vader_score(tweet)) return(np.array(values)) def fit(self, X, y=None): \"\"\"Returns `self` unless something different happens in train and test\"\"\" return self vader_feat = VaderFeatureExtractor(tokenizer) liu_feat = LiuFeatureExtractor(tokenizer) log_mod = LogisticRegression(solver='liblinear',multi_class='ovr') vader_liu_clf = Pipeline([ ('feats', FeatureUnion([ ('vader', vader_feat), ('liu',liu_feat) ])), ('clf', log_mod)]) vader_liu_clf.fit(train_data.tweet, train_data.sent) pred_vader_liu = vader_liu_clf.predict(test_data.tweet) conf_vader_liu = confusion_matrix(test_data.sent, pred_vader_liu) kappa_vader_liu = cohen_kappa_score(test_data.sent, pred_vader_liu) class_rep_vader_liu = classification_report(test_data.sent, pred_vader_liu) print('Confusion Matrix for Logistic Regression + Vader + features from Bing Liu\\'s Lexicon') print(conf_vader_liu) print('Classification Report') print(class_rep_vader_liu) print('kappa:'+str(kappa_vader_liu)) Results: Confusion Matrix for Logistic Regression + Vader + features from Bing Liu's Lexicon [[ 169 323 109] [ 51 1275 314] [ 58 491 1023]] Classification Report precision recall f1-score support negative 0.61 0.28 0.38 601 neutral 0.61 0.78 0.68 1640 positive 0.71 0.65 0.68 1572 avg / total 0.65 0.65 0.63 3813 kappa:0.408231856331834","title":"Logistic regression model using Bing Liu's Lexicon + Vader"},{"location":"benchmark/#logistic-regression-model-using-n-grams-bing-lius-lexicon-vader","text":"We now combine the feature space of all the previous examples: ngram_lex_clf = Pipeline([ ('feats', FeatureUnion([ ('ngram', vectorizer), ('vader',vader_feat),('liu',liu_feat) ])), ('clf', log_mod)]) ngram_lex_clf.fit(train_data.tweet, train_data.sent) pred_ngram_lex = ngram_lex_clf.predict(test_data.tweet) conf_ngram_lex = confusion_matrix(test_data.sent, pred_ngram_lex) kappa_ngram_lex = cohen_kappa_score(test_data.sent, pred_ngram_lex) class_rep = classification_report(test_data.sent, pred_ngram_lex) print('Confusion Matrix for Logistic Regression + ngrams + features from Bing Liu\\'s Lexicon and the Vader method') print(conf_ngram_lex) print('Classification Report') print(class_rep) print('kappa:'+str(kappa_ngram_lex)) Results: Confusion Matrix for Logistic Regression + ngrams + features from Bing Liu's Lexicon and the Vader method [[ 268 261 72] [ 45 1387 208] [ 56 493 1023]] Classification Report precision recall f1-score support negative 0.73 0.45 0.55 601 neutral 0.65 0.85 0.73 1640 positive 0.79 0.65 0.71 1572 avg / total 0.72 0.70 0.70 3813 kappa:0.5058311344923361","title":"Logistic regression model using n-grams + Bing Liu's Lexicon + Vader"},{"location":"benchmark/#summary-of-results","text":"A table summarising all the experiments from above is shown as follows: Features Implementation Kappa Score F1 Score Time (Seconds) Word n-grams Scikitlearn + NLTK 0.42 0.64 30.7 Word n-grams AffectiveTweets 0.45 0.66 13.0 Word n-grams + Liu Lexicon Scikitlearn + NLTK 0.48 0.68 13.4 Word n-grams + Liu Lexicon AffectiveTweets 0.48 0.68 27.4 Liu Lexicon + Vader Scikitlearn + NLTK 0.41 0.63 8.9 Liu Lexicon + SentiStrength AffectiveTweets 0.40 0.63 31.9 Word n-grams + Liu Lexicon + Vader Scikitlearn + NLTK 0.51 0.70 16 Word n-grams + Liu Lexicon + SentiStrength AffectiveTweets 0.49 0.69 68.5 Word n-grams + All lexicons + SentiStrength AffectiveTweets 0.52 0.71 74.6 The execution time is averaged over 10 repetitions of each model. Bear in mind that there are only two models (word n-grams and word n-grams+Liu Lexicon) that can be directly compared in both implementations (AffectiveTweets and Scikitlearn+NLTK) as they use the same features and the same learning schemes. Other examples such as Liu Lexicon+Vader and Liu Lexicon+SentiStregnth show how similar models can be implemented using two different tools. The experiments were performed on an Intel(R) Core(TM) i7-2600 CPU @ 3.40GHz with 16 GB of RAM using Ubuntu 16.04.4 LTS. AfftectiveTweets models were run using Weka 3.9.3 and Java 8 (Oracle version). Scikitlearn+NLTK models were run using Python 3.6.4 (Anaconda version), Scikitlearn 0.20.3 and NLTK 3.4.1.","title":"Summary of Results"},{"location":"contribute/","text":"Contributing Guidelines New contributors are more than welcome. If you want to contribute just fork the project and send a pull request with your changes. Weka Filter AffectiveTweets methods extend the Weka Filter class, particularly the SimpleBatchFilter class. Please read the instructions for implementing a Weka filter from here before continuing. Implementing a new AffectiveTweets Filter We will show how to implement a simple filter that adds a new numeric attribute to the given dataset. This attribute will count the number of times the words from a given list occur in a given tweet. The list is given as comma separated string. New filters can extend the TweetToFeatureVector abstract class to inherit tokenization and many other preprocessing functionalities useful for sentiment analysis of tweets (e.g., reduce repeated letters, standardize URLs) . /* * This program is free software: you can redistribute it and/or modify * it under the terms of the GNU General Public License as published by * the Free Software Foundation, either version 3 of the License, or * (at your option) any later version. * * This program is distributed in the hope that it will be useful, * but WITHOUT ANY WARRANTY; without even the implied warranty of * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the * GNU General Public License for more details. * * You should have received a copy of the GNU General Public License * along with this program. If not, see <http://www.gnu.org/licenses/>. */ /* * TweetToWordListCountFeatureVector.java * Copyright (C) 1999-2019 University of Waikato, Hamilton, New Zealand * */ package weka.filters.unsupervised.attribute; import java.util.ArrayList; import java.util.HashSet; import java.util.List; import java.util.Set; import java.util.Arrays; import weka.core.Attribute; import weka.core.Instance; import weka.core.Instances; import weka.core.OptionMetadata; import weka.core.SparseInstance; /** * * @author Felipe Bravo-Marquez (fbravoma@waikato.ac.nz) */ public class TweetToWordListCountFeatureVector extends TweetToFeatureVector { /** For serialization. */ private static final long serialVersionUID = -573366510055859430L; /** The given word list as a comma separated string. */ public String wordList = \"love,happy,great\"; /** * Returns a string describing this filter. * * @return a description of the filter suitable for displaying in the * explorer/experimenter gui */ @Override public String globalInfo() { return \"A simple filter that counts occurrences of words from a given list.\"; } /* (non-Javadoc) * @see weka.filters.SimpleFilter#determineOutputFormat(weka.core.Instances) */ @Override protected Instances determineOutputFormat(Instances inputFormat) throws Exception { ArrayList<Attribute> att = new ArrayList<Attribute>(); // Adds all attributes of the inputformat for (int i = 0; i < inputFormat.numAttributes(); i++) { att.add(inputFormat.attribute(i)); } // adds the new attribute att.add(new Attribute(\"wordListCount\")); Instances result = new Instances(inputFormat.relationName(), att, 0); // set the class index result.setClassIndex(inputFormat.classIndex()); return result; } /* (non-Javadoc) * @see weka.filters.SimpleFilter#process(weka.core.Instances) */ @Override protected Instances process(Instances instances) throws Exception { // set upper value for text index m_textIndex.setUpper(instances.numAttributes() - 1); Instances result = getOutputFormat(); // reference to the content of the message, users index start from zero Attribute attrCont = instances.attribute(this.m_textIndex.getIndex()); for (int i = 0; i < instances.numInstances(); i++) { // copy all attribute values from the original dataset double[] values = new double[result.numAttributes()]; for (int n = 0; n < instances.numAttributes(); n++) values[n] = instances.instance(i).value(n); String content = instances.instance(i).stringValue(attrCont); // tokenize the content List<String> words = affective.core.Utils.tokenize(content, this.toLowerCase, this.standarizeUrlsUsers, this.reduceRepeatedLetters, this.m_tokenizer,this.m_stemmer,this.m_stopwordsHandler); // convert the list of words into a HashSet Set<String> wordSet = new HashSet<String>(Arrays.asList(wordList.split(\",\"))); // count all the occurrences of words from the list int wordCounter = 0; for(String word:words){ if(wordSet.contains(word)) wordCounter++; } // add the value to the last attribute values[values.length - 1] = wordCounter; Instance inst = new SparseInstance(1, values); inst.setDataset(result); // copy possible strings, relational values... copyValues(inst, false, instances, result); result.add(inst); } return result; } /** * Main method for testing this class. * * @param args should contain arguments to the filter: use -h for help */ public static void main(String[] args) { runFilter(new TweetToWordListCountFeatureVector(), args); } // OptionMetada allows setting parameters from within the command-line interface @OptionMetadata(displayName = \"wordlist\", description = \"The list with the words to count separated by a comma symbol.\", commandLineParamName = \"wordlist\", commandLineParamSynopsis = \"-wordlist <string>\", displayOrder = 6) public String getWordList() { return wordList; } public void setWordList(String wordList) { this.wordList = wordList; } } One way to use this new filter class from within Weka, assuming the source code of the class is in the appropriate subfolder of the src folder of the AffectiveTweets project, is to rebuild and reinstall the AffectiveTweets package by using the project\u2019s build-package.xml file with the ant build tool. Implementing a JUnit Test To test the new filter we need to implement a JUnit test. A new filter test can extend AbstractFilterTest , which can be found in the Weka source code repository, as shown below: /* * This program is free software: you can redistribute it and/or modify * it under the terms of the GNU General Public License as published by * the Free Software Foundation, either version 3 of the License, or * (at your option) any later version. * * This program is distributed in the hope that it will be useful, * but WITHOUT ANY WARRANTY; without even the implied warranty of * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the * GNU General Public License for more details. * * You should have received a copy of the GNU General Public License * along with this program. If not, see <http://www.gnu.org/licenses/>. */ /* * Copyright (C) 2019 University of Waikato, Hamilton, New Zealand */ package weka.filters.unsupervised.attribute; import weka.classifiers.meta.FilteredClassifier; import weka.core.Instances; import weka.filters.AbstractFilterTest; import weka.filters.Filter; import junit.framework.Test; import junit.framework.TestSuite; import java.io.File; /** * Tests TweetToWordListCountFeatureVectorTest. Run from the command line with: <p/> * java weka.filters.unsupervised.attribute.TweetToWordListCountFeatureVectorTest * <p> * AffectiveTweets package must either be installed or * JVM must be started in AffectiveTweets directory. * <p> * @author FracPete and eibe * @version $Revision: 9568 $ */ public class TweetToWordListCountFeatureVectorTest extends AbstractFilterTest { public TweetToWordListCountFeatureVectorTest(String name) { super(name); } /** Creates a default TweetToSentiStrengthFeatureVector filter */ public Filter getFilter() { Filter f = null; // Check to see if the test is run from directory containing build_package.xml if ((new File(\"..\" + File.separator + \"AffectiveTweets\" + File.separator + \"build_package.xml\")).exists()) { File backup = weka.core.WekaPackageManager.PACKAGES_DIR; weka.core.WekaPackageManager.PACKAGES_DIR = new java.io.File(\"..\"); // So that default lexicon, etc., is found. f = new TweetToWordListCountFeatureVector(); weka.core.WekaPackageManager.PACKAGES_DIR = backup; } else { f = new TweetToWordListCountFeatureVector(); // Hope that the package is installed. } return f; } /** * Test for the FilteredClassifier used with this filter. * * @return the configured FilteredClassifier */ protected FilteredClassifier getFilteredClassifier() { FilteredClassifier result; result = new FilteredClassifier(); weka.filters.MultiFilter mf = new weka.filters.MultiFilter(); Filter[] filters = new Filter[2]; filters[0] = getFilter(); weka.filters.unsupervised.attribute.RemoveType rt = new weka.filters.unsupervised.attribute.RemoveType(); // Need to remove string attributes because they are kept by this filter. filters[1] = rt; mf.setFilters(filters); result.setFilter(mf); result.setClassifier(new weka.classifiers.functions.SMO()); return result; } /** * Data to be used for FilteredClassifier test. * * @return the configured FilteredClassifier */ protected Instances getFilteredClassifierData() throws Exception { Instances result; // Check to see if the test is run from directory containing build_package.xml if ((new File(\"..\" + File.separator + \"AffectiveTweets\" + File.separator + \"build_package.xml\")).exists()) { result = (new weka.core.converters.ConverterUtils.DataSource(\"data\" + File.separator + \"sent140test.arff.gz\")).getDataSet(); } else { // Hope that package is installed. result = (new weka.core.converters.ConverterUtils.DataSource(weka.core.WekaPackageManager.PACKAGES_DIR.toString() + File.separator + \"data\" + File.separator + \"sent140test.arff.gz\")).getDataSet(); } result.setClassIndex(result.numAttributes() - 1); return result; } /** * Called by JUnit before each test method. Sets up the Instances object to use based on * one of the datasets that comes with the package. * * @throws Exception if an error occurs reading the example instances. */ protected void setUp() throws Exception { super.setUp(); // Check to see if the test is run from directory containing build_package.xml if ((new File(\"..\" + File.separator + \"AffectiveTweets\" + File.separator + \"build_package.xml\")).exists()) { m_Instances = (new weka.core.converters.ConverterUtils.DataSource(\"data\" + File.separator + \"sent140test.arff.gz\")).getDataSet(); } else { // Hope that package is installed. m_Instances = (new weka.core.converters.ConverterUtils.DataSource(weka.core.WekaPackageManager.PACKAGES_DIR.toString() + File.separator + \"data\" + File.separator + \"sent140test.arff.gz\")).getDataSet(); } m_Instances.setClassIndex(m_Instances.numAttributes() - 1); } public static Test suite() { return new TestSuite(TweetToWordListCountFeatureVectorTest.class); } public static void main(String[] args){ junit.textui.TestRunner.run(suite()); } }","title":"Contributing"},{"location":"contribute/#contributing-guidelines","text":"New contributors are more than welcome. If you want to contribute just fork the project and send a pull request with your changes.","title":"Contributing Guidelines"},{"location":"contribute/#weka-filter","text":"AffectiveTweets methods extend the Weka Filter class, particularly the SimpleBatchFilter class. Please read the instructions for implementing a Weka filter from here before continuing.","title":"Weka Filter"},{"location":"contribute/#implementing-a-new-affectivetweets-filter","text":"We will show how to implement a simple filter that adds a new numeric attribute to the given dataset. This attribute will count the number of times the words from a given list occur in a given tweet. The list is given as comma separated string. New filters can extend the TweetToFeatureVector abstract class to inherit tokenization and many other preprocessing functionalities useful for sentiment analysis of tweets (e.g., reduce repeated letters, standardize URLs) . /* * This program is free software: you can redistribute it and/or modify * it under the terms of the GNU General Public License as published by * the Free Software Foundation, either version 3 of the License, or * (at your option) any later version. * * This program is distributed in the hope that it will be useful, * but WITHOUT ANY WARRANTY; without even the implied warranty of * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the * GNU General Public License for more details. * * You should have received a copy of the GNU General Public License * along with this program. If not, see <http://www.gnu.org/licenses/>. */ /* * TweetToWordListCountFeatureVector.java * Copyright (C) 1999-2019 University of Waikato, Hamilton, New Zealand * */ package weka.filters.unsupervised.attribute; import java.util.ArrayList; import java.util.HashSet; import java.util.List; import java.util.Set; import java.util.Arrays; import weka.core.Attribute; import weka.core.Instance; import weka.core.Instances; import weka.core.OptionMetadata; import weka.core.SparseInstance; /** * * @author Felipe Bravo-Marquez (fbravoma@waikato.ac.nz) */ public class TweetToWordListCountFeatureVector extends TweetToFeatureVector { /** For serialization. */ private static final long serialVersionUID = -573366510055859430L; /** The given word list as a comma separated string. */ public String wordList = \"love,happy,great\"; /** * Returns a string describing this filter. * * @return a description of the filter suitable for displaying in the * explorer/experimenter gui */ @Override public String globalInfo() { return \"A simple filter that counts occurrences of words from a given list.\"; } /* (non-Javadoc) * @see weka.filters.SimpleFilter#determineOutputFormat(weka.core.Instances) */ @Override protected Instances determineOutputFormat(Instances inputFormat) throws Exception { ArrayList<Attribute> att = new ArrayList<Attribute>(); // Adds all attributes of the inputformat for (int i = 0; i < inputFormat.numAttributes(); i++) { att.add(inputFormat.attribute(i)); } // adds the new attribute att.add(new Attribute(\"wordListCount\")); Instances result = new Instances(inputFormat.relationName(), att, 0); // set the class index result.setClassIndex(inputFormat.classIndex()); return result; } /* (non-Javadoc) * @see weka.filters.SimpleFilter#process(weka.core.Instances) */ @Override protected Instances process(Instances instances) throws Exception { // set upper value for text index m_textIndex.setUpper(instances.numAttributes() - 1); Instances result = getOutputFormat(); // reference to the content of the message, users index start from zero Attribute attrCont = instances.attribute(this.m_textIndex.getIndex()); for (int i = 0; i < instances.numInstances(); i++) { // copy all attribute values from the original dataset double[] values = new double[result.numAttributes()]; for (int n = 0; n < instances.numAttributes(); n++) values[n] = instances.instance(i).value(n); String content = instances.instance(i).stringValue(attrCont); // tokenize the content List<String> words = affective.core.Utils.tokenize(content, this.toLowerCase, this.standarizeUrlsUsers, this.reduceRepeatedLetters, this.m_tokenizer,this.m_stemmer,this.m_stopwordsHandler); // convert the list of words into a HashSet Set<String> wordSet = new HashSet<String>(Arrays.asList(wordList.split(\",\"))); // count all the occurrences of words from the list int wordCounter = 0; for(String word:words){ if(wordSet.contains(word)) wordCounter++; } // add the value to the last attribute values[values.length - 1] = wordCounter; Instance inst = new SparseInstance(1, values); inst.setDataset(result); // copy possible strings, relational values... copyValues(inst, false, instances, result); result.add(inst); } return result; } /** * Main method for testing this class. * * @param args should contain arguments to the filter: use -h for help */ public static void main(String[] args) { runFilter(new TweetToWordListCountFeatureVector(), args); } // OptionMetada allows setting parameters from within the command-line interface @OptionMetadata(displayName = \"wordlist\", description = \"The list with the words to count separated by a comma symbol.\", commandLineParamName = \"wordlist\", commandLineParamSynopsis = \"-wordlist <string>\", displayOrder = 6) public String getWordList() { return wordList; } public void setWordList(String wordList) { this.wordList = wordList; } } One way to use this new filter class from within Weka, assuming the source code of the class is in the appropriate subfolder of the src folder of the AffectiveTweets project, is to rebuild and reinstall the AffectiveTweets package by using the project\u2019s build-package.xml file with the ant build tool.","title":"Implementing a new AffectiveTweets Filter"},{"location":"contribute/#implementing-a-junit-test","text":"To test the new filter we need to implement a JUnit test. A new filter test can extend AbstractFilterTest , which can be found in the Weka source code repository, as shown below: /* * This program is free software: you can redistribute it and/or modify * it under the terms of the GNU General Public License as published by * the Free Software Foundation, either version 3 of the License, or * (at your option) any later version. * * This program is distributed in the hope that it will be useful, * but WITHOUT ANY WARRANTY; without even the implied warranty of * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the * GNU General Public License for more details. * * You should have received a copy of the GNU General Public License * along with this program. If not, see <http://www.gnu.org/licenses/>. */ /* * Copyright (C) 2019 University of Waikato, Hamilton, New Zealand */ package weka.filters.unsupervised.attribute; import weka.classifiers.meta.FilteredClassifier; import weka.core.Instances; import weka.filters.AbstractFilterTest; import weka.filters.Filter; import junit.framework.Test; import junit.framework.TestSuite; import java.io.File; /** * Tests TweetToWordListCountFeatureVectorTest. Run from the command line with: <p/> * java weka.filters.unsupervised.attribute.TweetToWordListCountFeatureVectorTest * <p> * AffectiveTweets package must either be installed or * JVM must be started in AffectiveTweets directory. * <p> * @author FracPete and eibe * @version $Revision: 9568 $ */ public class TweetToWordListCountFeatureVectorTest extends AbstractFilterTest { public TweetToWordListCountFeatureVectorTest(String name) { super(name); } /** Creates a default TweetToSentiStrengthFeatureVector filter */ public Filter getFilter() { Filter f = null; // Check to see if the test is run from directory containing build_package.xml if ((new File(\"..\" + File.separator + \"AffectiveTweets\" + File.separator + \"build_package.xml\")).exists()) { File backup = weka.core.WekaPackageManager.PACKAGES_DIR; weka.core.WekaPackageManager.PACKAGES_DIR = new java.io.File(\"..\"); // So that default lexicon, etc., is found. f = new TweetToWordListCountFeatureVector(); weka.core.WekaPackageManager.PACKAGES_DIR = backup; } else { f = new TweetToWordListCountFeatureVector(); // Hope that the package is installed. } return f; } /** * Test for the FilteredClassifier used with this filter. * * @return the configured FilteredClassifier */ protected FilteredClassifier getFilteredClassifier() { FilteredClassifier result; result = new FilteredClassifier(); weka.filters.MultiFilter mf = new weka.filters.MultiFilter(); Filter[] filters = new Filter[2]; filters[0] = getFilter(); weka.filters.unsupervised.attribute.RemoveType rt = new weka.filters.unsupervised.attribute.RemoveType(); // Need to remove string attributes because they are kept by this filter. filters[1] = rt; mf.setFilters(filters); result.setFilter(mf); result.setClassifier(new weka.classifiers.functions.SMO()); return result; } /** * Data to be used for FilteredClassifier test. * * @return the configured FilteredClassifier */ protected Instances getFilteredClassifierData() throws Exception { Instances result; // Check to see if the test is run from directory containing build_package.xml if ((new File(\"..\" + File.separator + \"AffectiveTweets\" + File.separator + \"build_package.xml\")).exists()) { result = (new weka.core.converters.ConverterUtils.DataSource(\"data\" + File.separator + \"sent140test.arff.gz\")).getDataSet(); } else { // Hope that package is installed. result = (new weka.core.converters.ConverterUtils.DataSource(weka.core.WekaPackageManager.PACKAGES_DIR.toString() + File.separator + \"data\" + File.separator + \"sent140test.arff.gz\")).getDataSet(); } result.setClassIndex(result.numAttributes() - 1); return result; } /** * Called by JUnit before each test method. Sets up the Instances object to use based on * one of the datasets that comes with the package. * * @throws Exception if an error occurs reading the example instances. */ protected void setUp() throws Exception { super.setUp(); // Check to see if the test is run from directory containing build_package.xml if ((new File(\"..\" + File.separator + \"AffectiveTweets\" + File.separator + \"build_package.xml\")).exists()) { m_Instances = (new weka.core.converters.ConverterUtils.DataSource(\"data\" + File.separator + \"sent140test.arff.gz\")).getDataSet(); } else { // Hope that package is installed. m_Instances = (new weka.core.converters.ConverterUtils.DataSource(weka.core.WekaPackageManager.PACKAGES_DIR.toString() + File.separator + \"data\" + File.separator + \"sent140test.arff.gz\")).getDataSet(); } m_Instances.setClassIndex(m_Instances.numAttributes() - 1); } public static Test suite() { return new TestSuite(TweetToWordListCountFeatureVectorTest.class); } public static void main(String[] args){ junit.textui.TestRunner.run(suite()); } }","title":"Implementing a JUnit Test"},{"location":"examples/","text":"The package can be used from the Weka GUI or the command line. Note: The following examples work with the newest version of the package. GUI Run WEKA and open the Explorer: java -Xmx4G -jar weka.jar Note: The -Xmx parameter allows incrementing the memory available for the Java virtual machine. It is strongly recommend to allocate as much memory as possible for large datasets or when calculating large dimensional features, such as word n-grams. More info here . Train an SVM using sparse features Open in the preprocess panel the sent140test.arff.gz dataset located in HOME/wekafiles/packages/AffectiveTweets/data/. Note: Select arff.gz files in the Files of Type option. Choose the TweetToSparseFeatureVector filter and configure it for calculating word n-grams, character n-grams, Brown word clusters, and POS tags: Train an SVM using LibLinear. Go to the classify panel and select the target class as the variable (Nom) class. Right click on the panel right to the Choose button and click on the Edit Configuration option . Paste the following snippet: weka.classifiers.meta.FilteredClassifier -F \"weka.filters.unsupervised.attribute.RemoveType -T string\" -W weka.classifiers.functions.LibLINEAR -- -S 1 -C 1.0 -E 0.001 -B 1.0 -L 0.1 -I 1000 Note: Weka allows copying and pasting the configuration of its objets. This is very convenient when training complex schemes with various parameters. The FilteredClassfier allows directly passing a filter to the classifier. In this example, we are removing the attributes of type string. Select the Percentage split option and start training the classifier. Note: This example is also shown in video 1 . Train an SVM using multiple affective lexicons, SentiStrength, and the average word-embedding vector Go back to the preprocess panel and press the Undo button to go back to the original dataset (or load the sent140test.arff.gz dataset in case you skipped the first example). Go to the Classify panel and paste the following snippet in the classifier's configuration: weka.classifiers.meta.FilteredClassifier -F \"weka.filters.MultiFilter -F \\\"weka.filters.unsupervised.attribute.TweetToInputLexiconFeatureVector -lexicon_evaluator \\\\\\\"affective.core.ArffLexiconEvaluator -lexiconFile $HOME/wekafiles/packages/AffectiveTweets/lexicons/arff_lexicons/NRC-AffectIntensity-Lexicon.arff -B NRC-Affect-Intensity -A 1 -lex-stemmer weka.core.stemmers.NullStemmer\\\\\\\" -red -stan -stemmer weka.core.stemmers.NullStemmer -stopwords-handler \\\\\\\"weka.core.stopwords.Null \\\\\\\" -I 1 -U -tokenizer \\\\\\\"weka.core.tokenizers.TweetNLPTokenizer \\\\\\\"\\\" -F \\\"weka.filters.unsupervised.attribute.TweetToSentiStrengthFeatureVector -L $HOME/wekafiles/packages/AffectiveTweets/lexicons/SentiStrength/english -red -stan -stemmer weka.core.stemmers.NullStemmer -stopwords-handler \\\\\\\"weka.core.stopwords.Null \\\\\\\" -I 1 -U -tokenizer \\\\\\\"weka.core.tokenizers.TweetNLPTokenizer \\\\\\\"\\\" -F \\\"weka.filters.unsupervised.attribute.TweetToLexiconFeatureVector -F -D -R -A -T -L -N -P -J -H -Q -red -stan -stemmer weka.core.stemmers.NullStemmer -stopwords-handler \\\\\\\"weka.core.stopwords.Null \\\\\\\" -I 1 -U -tokenizer \\\\\\\"weka.core.tokenizers.TweetNLPTokenizer \\\\\\\"\\\" -F \\\"weka.filters.unsupervised.attribute.TweetToEmbeddingsFeatureVector -S AVERAGE_ACTION -embeddingHandler \\\\\\\"affective.core.CSVEmbeddingHandler -K $HOME/wekafiles/packages/AffectiveTweets/resources/w2v.twitter.edinburgh.100d.csv.gz -sep TAB -I last\\\\\\\" -K 15 -red -stan -stemmer weka.core.stemmers.NullStemmer -stopwords-handler \\\\\\\"weka.core.stopwords.Null \\\\\\\" -I 1 -U -tokenizer \\\\\\\"weka.core.tokenizers.TweetNLPTokenizer \\\\\\\"\\\" -F \\\"weka.filters.unsupervised.attribute.Reorder -R 4-last,3\\\"\" -S 1 -W weka.classifiers.functions.LibLINEAR -- -S 1 -C 1.0 -E 0.001 -B 1.0 -L 0.1 -I 1000 Note: replace $HOME by your home directory (e.g., /home/felipe). We are using the MultiFilter filter to nest multiple filters. The Reorder filter is used to discard the first two String attributes and moving the class label to the last position. Now you can train the classifier by pressing the Start button. Note: This example is also shown in video 1 . Create a Lexicon of sentiment words using the TweetCentroid method Open in the preprocess panel the unlabelled.arff.gz dataset of unlabelled tweets. Train word vectors using the tweet centroid model using the TweetCentroid filter. Paste the following snippet: weka.filters.unsupervised.attribute.TweetCentroid -C -W -F -natt -M 10 -N 10 -H $HOME/wekafiles/packages/AffectiveTweets/resources/50mpaths2.txt.gz -stemmer weka.core.stemmers.NullStemmer -stopwords-handler \"weka.core.stopwords.Null \" -I 1 -U -tokenizer \"weka.core.tokenizers.TweetNLPTokenizer \" Label the resulting word vectors with a seed lexicon in arff format using the LabelWordVector Filter: weka.filters.unsupervised.attribute.LabelWordVectors -lexicon_evaluator \"affective.core.ArffLexiconWordLabeller -lexiconFile $HOME/wekafiles/packages/AffectiveTweets/lexicons/arff_lexicons/metaLexEmo.arff -B MetaLexEmo -A 1 -lex-stemmer weka.core.stemmers.NullStemmer\" -U -I last Train a classifier a logistic regression on labelled words and add predictions as new attributes using the AddClassification filter: weka.filters.supervised.attribute.AddClassification -remove-old-class -distribution -W \"weka.classifiers.meta.FilteredClassifier -F \\\"weka.filters.unsupervised.attribute.RemoveType -T string\\\" -W weka.classifiers.functions.LibLINEAR -- -S 7 -C 1.0 -E 0.001 -B 1.0 -P -L 0.1 -I 1000\" Remove all the word attributes to create a lexicon: weka.filters.unsupervised.attribute.Remove -R first-4121 Save the resulting lexicon as an arff file by clicking on the save button. Use your new lexicon on a different tweet dataset using the TweetToInputLexiconFeatureVector filter. Note: This example is also shown in video 2 . Create a Lexicon of sentiment words using PMI Semantic Orientation Open in the preprocess panel the sent140train.arff.gz dataset. This is a large corpus, so make sure to increase the heap size when running Weka. Create a PMI lexicon using the PMILexiconExpander filter with default parameters. This is a supervised filter. Save the lexicon as an arff file and use it with the TweetToInputLexiconFeatureVector filter. Note: This example is also shown in video 2 . Train a Tweet-level polarity classifier from unlabelled tweets using emoticon labels Distant supervision is very useful when tweets annotated by sentiment are not available. In this example we will show how to train a classifier using emoticons as noisy labels. Open in the preprocess panel the unlabelled.arff.gz dataset of unlabelled tweets. Label tweets based on the polarity of emoticons (tweets without emoticons will be discarded): weka.filters.unsupervised.attribute.LexiconDistantSupervision -lex $HOME/wekafiles/packages/AffectiveTweets/lexicons/arff_lexicons/emoticons.arff -polatt polarity -negval negative -posval positive -removeMatchingWord -I 1 -tokenizer \"weka.core.tokenizers.TweetNLPTokenizer \" Rename the polarity label to class (this is needed to make the data compatible with the testing set): weka.filters.unsupervised.attribute.RenameAttribute -find polarity -replace class -R last Train a classifier using unigram as features and deploy the classifier on target annotated tweets. Go to the classsify panel and set the file 6HumanPosNeg.arff.gz as the supplied test set. Next, paste the following snippet in the classify panel: weka.classifiers.meta.FilteredClassifier -F \"weka.filters.MultiFilter -F \\\"weka.filters.unsupervised.attribute.TweetToSparseFeatureVector -E 5 -D 3 -I 0 -F -M 0 -G 0 -taggerFile $HOME/wekafiles/packages/AffectiveTweets/resources/model.20120919 -wordClustFile $HOME/wekafiles/packages/AffectiveTweets/resources/50mpaths2.txt.gz -Q 1 -stemmer weka.core.stemmers.NullStemmer -stopwords-handler \\\\\\\"weka.core.stopwords.Null \\\\\\\" -I 1 -U -tokenizer \\\\\\\"weka.core.tokenizers.TweetNLPTokenizer \\\\\\\"\\\" -F \\\"weka.filters.unsupervised.attribute.Reorder -R 3-last,2\\\"\" -W weka.classifiers.functions.LibLINEAR -- -S 7 -C 1.0 -E 0.001 -B 1.0 -P -L 0.1 -I 1000 Note: This example is also shown in video 3 . Train a Tweet-level polarity classifier from unlabelled tweets using the ASA and PTCM distant supervision methods In this example we will generate positive and negative instances from a corpus of unlabelled tweets using the ASA and the PTCM methods. The classifier wil be evaluated on positive and negative tweets. Open in the preprocess panel the unlabelled.arff.gz dataset of unlabelled tweets. Add a class label with negative and positive values using the Add filter in the preprocess panel: weka.filters.unsupervised.attribute.Add -T NOM -N class -L negative,positive -C last Note that the values for the class are empty for all instances. We are adding these labels to make the data compatible with the target tweets on which the classifier we will train will be deployed. Generate positive and negative instances using ASA and the BingLiu lexicon, then train a logistic regression on those instances, and deploy this classifier on the tweets from 6HumanPosNeg.arff.gz . Go to the classsify panel and set the file 6HumanPosNeg.arff.gz as the supplied test set. Next, paste the following snippet in the classify panel: weka.classifiers.meta.FilteredClassifier -F \"weka.filters.unsupervised.attribute.ASA -C -W -lex $HOME/wekafiles/packages/AffectiveTweets/lexicons/arff_lexicons/BingLiu.arff -M 10 -nneg 1000 -npos 1000 -polatt polarity -negval negative -posval positive -R 1 -A 10 -H $HOME/wekafiles/packages/AffectiveTweets/resources/50mpaths2.txt.gz -stemmer weka.core.stemmers.NullStemmer -stopwords-handler \\\"weka.core.stopwords.Null \\\" -I 1 -U -tokenizer \\\"weka.core.tokenizers.TweetNLPTokenizer \\\"\" -W weka.classifiers.functions.LibLINEAR -- -S 7 -C 1.0 -E 0.001 -B 1.0 -P -L 0.1 -I 1000 Paste the following snippet for using the PTCM. The partition size for the word vectors is set to 4: weka.classifiers.meta.FilteredClassifier -F \"weka.filters.unsupervised.attribute.PTCM -C -W -lex $HOME/wekafiles/packages/AffectiveTweets/lexicons/arff_lexicons/BingLiu.arff -M 4 -N 4 -A 10 -H /Users/admin/wekafiles/packages/AffectiveTweets/resources/50mpaths2.txt.gz -stemmer weka.core.stemmers.NullStemmer -stopwords-handler \\\"weka.core.stopwords.Null \\\" -I 1 -U -tokenizer \\\"weka.core.tokenizers.TweetNLPTokenizer \\\"\" -W weka.classifiers.functions.LibLINEAR -- -S 7 -C 1.0 -E 0.001 -B 1.0 -P -L 0.1 -I 1000 Note: These examples are also shown in video 3 . Command-line The same classification schemes can be run from the command line. Tweet Classification from the CL An example using various types of attributes is given below: java -Xmx4G -cp weka.jar weka.Run weka.classifiers.meta.FilteredClassifier -t $HOME/wekafiles/packages/AffectiveTweets/data/sent140test.arff.gz -split-percentage 66 -F \"weka.filters.MultiFilter -F \\\"weka.filters.unsupervised.attribute.TweetToInputLexiconFeatureVector -lexicon_evaluator \\\\\\\"affective.core.ArffLexiconEvaluator -lexiconFile $HOME/wekafiles/packages/AffectiveTweets/lexicons/arff_lexicons/NRC-AffectIntensity-Lexicon.arff -B NRC-Affect-Intensity -A 1 -lex-stemmer weka.core.stemmers.NullStemmer\\\\\\\" -red -stan -stemmer weka.core.stemmers.NullStemmer -stopwords-handler \\\\\\\"weka.core.stopwords.Null \\\\\\\" -I 1 -U -tokenizer \\\\\\\"weka.core.tokenizers.TweetNLPTokenizer \\\\\\\"\\\" -F \\\"weka.filters.unsupervised.attribute.TweetToSentiStrengthFeatureVector -L $HOME/wekafiles/packages/AffectiveTweets/lexicons/SentiStrength/english -red -stan -stemmer weka.core.stemmers.NullStemmer -stopwords-handler \\\\\\\"weka.core.stopwords.Null \\\\\\\" -I 1 -U -tokenizer \\\\\\\"weka.core.tokenizers.TweetNLPTokenizer \\\\\\\"\\\" -F \\\"weka.filters.unsupervised.attribute.TweetToLexiconFeatureVector -F -D -R -A -T -L -N -P -J -H -Q -red -stan -stemmer weka.core.stemmers.NullStemmer -stopwords-handler \\\\\\\"weka.core.stopwords.Null \\\\\\\" -I 1 -U -tokenizer \\\\\\\"weka.core.tokenizers.TweetNLPTokenizer \\\\\\\"\\\" -F \\\"weka.filters.unsupervised.attribute.TweetToEmbeddingsFeatureVector -S AVERAGE_ACTION -embeddingHandler \\\\\\\"affective.core.CSVEmbeddingHandler -K $HOME/wekafiles/packages/AffectiveTweets/resources/w2v.twitter.edinburgh.100d.csv.gz -sep TAB -I last\\\\\\\" -K 15 -red -stan -stemmer weka.core.stemmers.NullStemmer -stopwords-handler \\\\\\\"weka.core.stopwords.Null \\\\\\\" -I 1 -U -tokenizer \\\\\\\"weka.core.tokenizers.TweetNLPTokenizer \\\\\\\"\\\" -F \\\"weka.filters.unsupervised.attribute.Reorder -R 4-last,3\\\"\" -S 1 -W weka.classifiers.functions.LibLINEAR -- -S 1 -C 1.0 -E 0.001 -B 1.0 -L 0.1 -I 1000 Feature Extraction from the CL There is also possible to run filters in isolation and then convert the processed files into CSV files: First run a filter: java -Xmx4G -cp weka.jar weka.Run weka.filters.unsupervised.attribute.TweetToInputLexiconFeatureVector -i $HOME/wekafiles/packages/AffectiveTweets/data/sent140test.arff.gz -o proc_data.arff -lexicon_evaluator \"affective.core.ArffLexiconEvaluator -lexiconFile /Users/admin/wekafiles/packages/AffectiveTweets/lexicons/arff_lexicons/NRC-AffectIntensity-Lexicon.arff -B NRC-Affect-Intensity -A 1 -lex-stemmer weka.core.stemmers.NullStemmer\" -stemmer weka.core.stemmers.NullStemmer -stopwords-handler \"weka.core.stopwords.Null \" -I 1 -U -tokenizer \"weka.core.tokenizers.TweetNLPTokenizer \" Then, convert the new feature vector into a CSV file. java -Xmx4G -cp weka.jar weka.core.converters.CSVSaver -i proc_data.arff -o proc_data.csv More information about how to run filters from the command line on the test data can be found here . AffectiveTweets and Deep Learning In order to train deep learning models with AffectiveTweets you first need to install the WekaDeepLearning4j package, which is a wrapper of the DeepLearning4j library. The package can be installed by following the instructions from here . Some examples instructions using the two packages together are given below. Create a Lexicon of Sentiment words using Word2Vec and Glove. The WekaDeepLearning4j package implements two filters for calculating word vectors (or word embeddings) using modern bi-linear neural networks models: Dl4jStringToWord2Vec : calculates word embeddings on a string attribute using the Word2Vec method Dl4jStringToGlove : calculates word embeddings on a string attribute using the Glove method. Use these filters in an analogous way as the TweetCentroid filter and then following the same steps as the example from here . Train a Convolution Neural Network on the concatenation of word embeddings In this example we will show how to train a convolution neural network with a similar arquitecture to the one used in this paper from the Weka GUI. Represent each tweet from the sent140test.arff.gz dataset as a sequence of its first 15 word embeddings by pasting the following filter configuration: weka.filters.unsupervised.attribute.TweetToEmbeddingsFeatureVector -S CONCATENATE_ACTION -embeddingHandler \"affective.core.CSVEmbeddingHandler -K $HOME/wekafiles/packages/AffectiveTweets/resources/w2v.twitter.edinburgh.100d.csv.gz -sep TAB -I last\" -K 15 -stemmer weka.core.stemmers.NullStemmer -stopwords-handler \"weka.core.stopwords.Null \" -I 1 -U -tokenizer \"weka.core.tokenizers.TweetNLPTokenizer \" Discard the string content and move the class label to the last position: weka.filters.unsupervised.attribute.Reorder -R 4-last,3 Train a convolutional neural network using a Dl4jMlpClassifier . Paste the following snippet in the Classification panel: weka.classifiers.functions.Dl4jMlpClassifier -S 1 -iterator \"weka.dl4j.iterators.ConvolutionalInstancesIterator -height 1 -numChannels 1 -bs 256 -width 1500\" -layers \"weka.dl4j.layers.ConvolutionLayer -nFilters 100 -activation identity -adamMeanDecay 0.9 -adamVarDecay 0.999 -biasInit 1.0 -biasL1 0.0 -biasL2 0.0 -blr 0.01 -mode Truncate -cudnnAlgoMode PREFER_FASTEST -dist \\\"weka.dl4j.distribution.NormalDistribution -mean 0.001 -std 1.0\\\" -dropout 0.0 -epsilon 1.0E-6 -gradientNormalization None -gradNormThreshold 1.0 -kernelSizeX 300 -kernelSizeY 1 -L1 0.0 -L2 0.0 -name \\\"Convolution layer\\\" -lr 0.01 -momentum 0.9 -paddingX 0 -paddingY 0 -rho 0.0 -rmsDecay 0.95 -strideX 100 -strideY 1 -updater NESTEROVS -weightInit XAVIER\" -layers \"weka.dl4j.layers.OutputLayer -activation softmax -adamMeanDecay 0.9 -adamVarDecay 0.999 -biasInit 1.0 -biasL1 0.0 -biasL2 0.0 -blr 0.01 -dist \\\"weka.dl4j.distribution.NormalDistribution -mean 0.001 -std 1.0\\\" -dropout 0.0 -epsilon 1.0E-6 -gradientNormalization None -gradNormThreshold 1.0 -L1 0.0 -L2 0.0 -name \\\"Output layer\\\" -lr 0.01 -lossFn LossMCXENT() -momentum 0.9 -rho 0.0 -rmsDecay 0.95 -updater NESTEROVS -weightInit XAVIER\" -logFile weka.log -numEpochs 200 -algorithm STOCHASTIC_GRADIENT_DESCENT Note: This code is not compatible with the latest version of the WekaDeepLearning4j package. This network has 100 filters in a convolutional layer, followed by the output layer. The filter size is 300x1 (i.e, each filter maps a word trigram, since each word has 100 dimensions). The stride is 100x1 (the number of dimensions for a word). The number of epochs is 200. The input width is 1500 and the input height is 1. The number of input channels is 1 and the batch size is 256.","title":"Examples"},{"location":"examples/#gui","text":"Run WEKA and open the Explorer: java -Xmx4G -jar weka.jar Note: The -Xmx parameter allows incrementing the memory available for the Java virtual machine. It is strongly recommend to allocate as much memory as possible for large datasets or when calculating large dimensional features, such as word n-grams. More info here .","title":"GUI"},{"location":"examples/#train-an-svm-using-sparse-features","text":"Open in the preprocess panel the sent140test.arff.gz dataset located in HOME/wekafiles/packages/AffectiveTweets/data/. Note: Select arff.gz files in the Files of Type option. Choose the TweetToSparseFeatureVector filter and configure it for calculating word n-grams, character n-grams, Brown word clusters, and POS tags: Train an SVM using LibLinear. Go to the classify panel and select the target class as the variable (Nom) class. Right click on the panel right to the Choose button and click on the Edit Configuration option . Paste the following snippet: weka.classifiers.meta.FilteredClassifier -F \"weka.filters.unsupervised.attribute.RemoveType -T string\" -W weka.classifiers.functions.LibLINEAR -- -S 1 -C 1.0 -E 0.001 -B 1.0 -L 0.1 -I 1000 Note: Weka allows copying and pasting the configuration of its objets. This is very convenient when training complex schemes with various parameters. The FilteredClassfier allows directly passing a filter to the classifier. In this example, we are removing the attributes of type string. Select the Percentage split option and start training the classifier. Note: This example is also shown in video 1 .","title":"Train an SVM using sparse features"},{"location":"examples/#train-an-svm-using-multiple-affective-lexicons-sentistrength-and-the-average-word-embedding-vector","text":"Go back to the preprocess panel and press the Undo button to go back to the original dataset (or load the sent140test.arff.gz dataset in case you skipped the first example). Go to the Classify panel and paste the following snippet in the classifier's configuration: weka.classifiers.meta.FilteredClassifier -F \"weka.filters.MultiFilter -F \\\"weka.filters.unsupervised.attribute.TweetToInputLexiconFeatureVector -lexicon_evaluator \\\\\\\"affective.core.ArffLexiconEvaluator -lexiconFile $HOME/wekafiles/packages/AffectiveTweets/lexicons/arff_lexicons/NRC-AffectIntensity-Lexicon.arff -B NRC-Affect-Intensity -A 1 -lex-stemmer weka.core.stemmers.NullStemmer\\\\\\\" -red -stan -stemmer weka.core.stemmers.NullStemmer -stopwords-handler \\\\\\\"weka.core.stopwords.Null \\\\\\\" -I 1 -U -tokenizer \\\\\\\"weka.core.tokenizers.TweetNLPTokenizer \\\\\\\"\\\" -F \\\"weka.filters.unsupervised.attribute.TweetToSentiStrengthFeatureVector -L $HOME/wekafiles/packages/AffectiveTweets/lexicons/SentiStrength/english -red -stan -stemmer weka.core.stemmers.NullStemmer -stopwords-handler \\\\\\\"weka.core.stopwords.Null \\\\\\\" -I 1 -U -tokenizer \\\\\\\"weka.core.tokenizers.TweetNLPTokenizer \\\\\\\"\\\" -F \\\"weka.filters.unsupervised.attribute.TweetToLexiconFeatureVector -F -D -R -A -T -L -N -P -J -H -Q -red -stan -stemmer weka.core.stemmers.NullStemmer -stopwords-handler \\\\\\\"weka.core.stopwords.Null \\\\\\\" -I 1 -U -tokenizer \\\\\\\"weka.core.tokenizers.TweetNLPTokenizer \\\\\\\"\\\" -F \\\"weka.filters.unsupervised.attribute.TweetToEmbeddingsFeatureVector -S AVERAGE_ACTION -embeddingHandler \\\\\\\"affective.core.CSVEmbeddingHandler -K $HOME/wekafiles/packages/AffectiveTweets/resources/w2v.twitter.edinburgh.100d.csv.gz -sep TAB -I last\\\\\\\" -K 15 -red -stan -stemmer weka.core.stemmers.NullStemmer -stopwords-handler \\\\\\\"weka.core.stopwords.Null \\\\\\\" -I 1 -U -tokenizer \\\\\\\"weka.core.tokenizers.TweetNLPTokenizer \\\\\\\"\\\" -F \\\"weka.filters.unsupervised.attribute.Reorder -R 4-last,3\\\"\" -S 1 -W weka.classifiers.functions.LibLINEAR -- -S 1 -C 1.0 -E 0.001 -B 1.0 -L 0.1 -I 1000 Note: replace $HOME by your home directory (e.g., /home/felipe). We are using the MultiFilter filter to nest multiple filters. The Reorder filter is used to discard the first two String attributes and moving the class label to the last position. Now you can train the classifier by pressing the Start button. Note: This example is also shown in video 1 .","title":"Train an SVM using multiple affective lexicons, SentiStrength, and the average word-embedding vector"},{"location":"examples/#create-a-lexicon-of-sentiment-words-using-the-tweetcentroid-method","text":"Open in the preprocess panel the unlabelled.arff.gz dataset of unlabelled tweets. Train word vectors using the tweet centroid model using the TweetCentroid filter. Paste the following snippet: weka.filters.unsupervised.attribute.TweetCentroid -C -W -F -natt -M 10 -N 10 -H $HOME/wekafiles/packages/AffectiveTweets/resources/50mpaths2.txt.gz -stemmer weka.core.stemmers.NullStemmer -stopwords-handler \"weka.core.stopwords.Null \" -I 1 -U -tokenizer \"weka.core.tokenizers.TweetNLPTokenizer \" Label the resulting word vectors with a seed lexicon in arff format using the LabelWordVector Filter: weka.filters.unsupervised.attribute.LabelWordVectors -lexicon_evaluator \"affective.core.ArffLexiconWordLabeller -lexiconFile $HOME/wekafiles/packages/AffectiveTweets/lexicons/arff_lexicons/metaLexEmo.arff -B MetaLexEmo -A 1 -lex-stemmer weka.core.stemmers.NullStemmer\" -U -I last Train a classifier a logistic regression on labelled words and add predictions as new attributes using the AddClassification filter: weka.filters.supervised.attribute.AddClassification -remove-old-class -distribution -W \"weka.classifiers.meta.FilteredClassifier -F \\\"weka.filters.unsupervised.attribute.RemoveType -T string\\\" -W weka.classifiers.functions.LibLINEAR -- -S 7 -C 1.0 -E 0.001 -B 1.0 -P -L 0.1 -I 1000\" Remove all the word attributes to create a lexicon: weka.filters.unsupervised.attribute.Remove -R first-4121 Save the resulting lexicon as an arff file by clicking on the save button. Use your new lexicon on a different tweet dataset using the TweetToInputLexiconFeatureVector filter. Note: This example is also shown in video 2 .","title":"Create a Lexicon of sentiment words using the TweetCentroid method"},{"location":"examples/#create-a-lexicon-of-sentiment-words-using-pmi-semantic-orientation","text":"Open in the preprocess panel the sent140train.arff.gz dataset. This is a large corpus, so make sure to increase the heap size when running Weka. Create a PMI lexicon using the PMILexiconExpander filter with default parameters. This is a supervised filter. Save the lexicon as an arff file and use it with the TweetToInputLexiconFeatureVector filter. Note: This example is also shown in video 2 .","title":"Create a Lexicon of sentiment words using PMI Semantic Orientation"},{"location":"examples/#train-a-tweet-level-polarity-classifier-from-unlabelled-tweets-using-emoticon-labels","text":"Distant supervision is very useful when tweets annotated by sentiment are not available. In this example we will show how to train a classifier using emoticons as noisy labels. Open in the preprocess panel the unlabelled.arff.gz dataset of unlabelled tweets. Label tweets based on the polarity of emoticons (tweets without emoticons will be discarded): weka.filters.unsupervised.attribute.LexiconDistantSupervision -lex $HOME/wekafiles/packages/AffectiveTweets/lexicons/arff_lexicons/emoticons.arff -polatt polarity -negval negative -posval positive -removeMatchingWord -I 1 -tokenizer \"weka.core.tokenizers.TweetNLPTokenizer \" Rename the polarity label to class (this is needed to make the data compatible with the testing set): weka.filters.unsupervised.attribute.RenameAttribute -find polarity -replace class -R last Train a classifier using unigram as features and deploy the classifier on target annotated tweets. Go to the classsify panel and set the file 6HumanPosNeg.arff.gz as the supplied test set. Next, paste the following snippet in the classify panel: weka.classifiers.meta.FilteredClassifier -F \"weka.filters.MultiFilter -F \\\"weka.filters.unsupervised.attribute.TweetToSparseFeatureVector -E 5 -D 3 -I 0 -F -M 0 -G 0 -taggerFile $HOME/wekafiles/packages/AffectiveTweets/resources/model.20120919 -wordClustFile $HOME/wekafiles/packages/AffectiveTweets/resources/50mpaths2.txt.gz -Q 1 -stemmer weka.core.stemmers.NullStemmer -stopwords-handler \\\\\\\"weka.core.stopwords.Null \\\\\\\" -I 1 -U -tokenizer \\\\\\\"weka.core.tokenizers.TweetNLPTokenizer \\\\\\\"\\\" -F \\\"weka.filters.unsupervised.attribute.Reorder -R 3-last,2\\\"\" -W weka.classifiers.functions.LibLINEAR -- -S 7 -C 1.0 -E 0.001 -B 1.0 -P -L 0.1 -I 1000 Note: This example is also shown in video 3 .","title":"Train a Tweet-level polarity classifier from unlabelled tweets using emoticon labels"},{"location":"examples/#train-a-tweet-level-polarity-classifier-from-unlabelled-tweets-using-the-asa-and-ptcm-distant-supervision-methods","text":"In this example we will generate positive and negative instances from a corpus of unlabelled tweets using the ASA and the PTCM methods. The classifier wil be evaluated on positive and negative tweets. Open in the preprocess panel the unlabelled.arff.gz dataset of unlabelled tweets. Add a class label with negative and positive values using the Add filter in the preprocess panel: weka.filters.unsupervised.attribute.Add -T NOM -N class -L negative,positive -C last Note that the values for the class are empty for all instances. We are adding these labels to make the data compatible with the target tweets on which the classifier we will train will be deployed. Generate positive and negative instances using ASA and the BingLiu lexicon, then train a logistic regression on those instances, and deploy this classifier on the tweets from 6HumanPosNeg.arff.gz . Go to the classsify panel and set the file 6HumanPosNeg.arff.gz as the supplied test set. Next, paste the following snippet in the classify panel: weka.classifiers.meta.FilteredClassifier -F \"weka.filters.unsupervised.attribute.ASA -C -W -lex $HOME/wekafiles/packages/AffectiveTweets/lexicons/arff_lexicons/BingLiu.arff -M 10 -nneg 1000 -npos 1000 -polatt polarity -negval negative -posval positive -R 1 -A 10 -H $HOME/wekafiles/packages/AffectiveTweets/resources/50mpaths2.txt.gz -stemmer weka.core.stemmers.NullStemmer -stopwords-handler \\\"weka.core.stopwords.Null \\\" -I 1 -U -tokenizer \\\"weka.core.tokenizers.TweetNLPTokenizer \\\"\" -W weka.classifiers.functions.LibLINEAR -- -S 7 -C 1.0 -E 0.001 -B 1.0 -P -L 0.1 -I 1000 Paste the following snippet for using the PTCM. The partition size for the word vectors is set to 4: weka.classifiers.meta.FilteredClassifier -F \"weka.filters.unsupervised.attribute.PTCM -C -W -lex $HOME/wekafiles/packages/AffectiveTweets/lexicons/arff_lexicons/BingLiu.arff -M 4 -N 4 -A 10 -H /Users/admin/wekafiles/packages/AffectiveTweets/resources/50mpaths2.txt.gz -stemmer weka.core.stemmers.NullStemmer -stopwords-handler \\\"weka.core.stopwords.Null \\\" -I 1 -U -tokenizer \\\"weka.core.tokenizers.TweetNLPTokenizer \\\"\" -W weka.classifiers.functions.LibLINEAR -- -S 7 -C 1.0 -E 0.001 -B 1.0 -P -L 0.1 -I 1000 Note: These examples are also shown in video 3 .","title":"Train a Tweet-level polarity classifier from unlabelled tweets using the ASA and PTCM distant supervision methods"},{"location":"examples/#command-line","text":"The same classification schemes can be run from the command line.","title":"Command-line"},{"location":"examples/#tweet-classification-from-the-cl","text":"An example using various types of attributes is given below: java -Xmx4G -cp weka.jar weka.Run weka.classifiers.meta.FilteredClassifier -t $HOME/wekafiles/packages/AffectiveTweets/data/sent140test.arff.gz -split-percentage 66 -F \"weka.filters.MultiFilter -F \\\"weka.filters.unsupervised.attribute.TweetToInputLexiconFeatureVector -lexicon_evaluator \\\\\\\"affective.core.ArffLexiconEvaluator -lexiconFile $HOME/wekafiles/packages/AffectiveTweets/lexicons/arff_lexicons/NRC-AffectIntensity-Lexicon.arff -B NRC-Affect-Intensity -A 1 -lex-stemmer weka.core.stemmers.NullStemmer\\\\\\\" -red -stan -stemmer weka.core.stemmers.NullStemmer -stopwords-handler \\\\\\\"weka.core.stopwords.Null \\\\\\\" -I 1 -U -tokenizer \\\\\\\"weka.core.tokenizers.TweetNLPTokenizer \\\\\\\"\\\" -F \\\"weka.filters.unsupervised.attribute.TweetToSentiStrengthFeatureVector -L $HOME/wekafiles/packages/AffectiveTweets/lexicons/SentiStrength/english -red -stan -stemmer weka.core.stemmers.NullStemmer -stopwords-handler \\\\\\\"weka.core.stopwords.Null \\\\\\\" -I 1 -U -tokenizer \\\\\\\"weka.core.tokenizers.TweetNLPTokenizer \\\\\\\"\\\" -F \\\"weka.filters.unsupervised.attribute.TweetToLexiconFeatureVector -F -D -R -A -T -L -N -P -J -H -Q -red -stan -stemmer weka.core.stemmers.NullStemmer -stopwords-handler \\\\\\\"weka.core.stopwords.Null \\\\\\\" -I 1 -U -tokenizer \\\\\\\"weka.core.tokenizers.TweetNLPTokenizer \\\\\\\"\\\" -F \\\"weka.filters.unsupervised.attribute.TweetToEmbeddingsFeatureVector -S AVERAGE_ACTION -embeddingHandler \\\\\\\"affective.core.CSVEmbeddingHandler -K $HOME/wekafiles/packages/AffectiveTweets/resources/w2v.twitter.edinburgh.100d.csv.gz -sep TAB -I last\\\\\\\" -K 15 -red -stan -stemmer weka.core.stemmers.NullStemmer -stopwords-handler \\\\\\\"weka.core.stopwords.Null \\\\\\\" -I 1 -U -tokenizer \\\\\\\"weka.core.tokenizers.TweetNLPTokenizer \\\\\\\"\\\" -F \\\"weka.filters.unsupervised.attribute.Reorder -R 4-last,3\\\"\" -S 1 -W weka.classifiers.functions.LibLINEAR -- -S 1 -C 1.0 -E 0.001 -B 1.0 -L 0.1 -I 1000","title":"Tweet Classification from the CL"},{"location":"examples/#feature-extraction-from-the-cl","text":"There is also possible to run filters in isolation and then convert the processed files into CSV files: First run a filter: java -Xmx4G -cp weka.jar weka.Run weka.filters.unsupervised.attribute.TweetToInputLexiconFeatureVector -i $HOME/wekafiles/packages/AffectiveTweets/data/sent140test.arff.gz -o proc_data.arff -lexicon_evaluator \"affective.core.ArffLexiconEvaluator -lexiconFile /Users/admin/wekafiles/packages/AffectiveTweets/lexicons/arff_lexicons/NRC-AffectIntensity-Lexicon.arff -B NRC-Affect-Intensity -A 1 -lex-stemmer weka.core.stemmers.NullStemmer\" -stemmer weka.core.stemmers.NullStemmer -stopwords-handler \"weka.core.stopwords.Null \" -I 1 -U -tokenizer \"weka.core.tokenizers.TweetNLPTokenizer \" Then, convert the new feature vector into a CSV file. java -Xmx4G -cp weka.jar weka.core.converters.CSVSaver -i proc_data.arff -o proc_data.csv More information about how to run filters from the command line on the test data can be found here .","title":"Feature Extraction from the CL"},{"location":"examples/#affectivetweets-and-deep-learning","text":"In order to train deep learning models with AffectiveTweets you first need to install the WekaDeepLearning4j package, which is a wrapper of the DeepLearning4j library. The package can be installed by following the instructions from here . Some examples instructions using the two packages together are given below.","title":"AffectiveTweets and Deep Learning"},{"location":"examples/#create-a-lexicon-of-sentiment-words-using-word2vec-and-glove","text":"The WekaDeepLearning4j package implements two filters for calculating word vectors (or word embeddings) using modern bi-linear neural networks models: Dl4jStringToWord2Vec : calculates word embeddings on a string attribute using the Word2Vec method Dl4jStringToGlove : calculates word embeddings on a string attribute using the Glove method. Use these filters in an analogous way as the TweetCentroid filter and then following the same steps as the example from here .","title":"Create a Lexicon of Sentiment words using Word2Vec and Glove."},{"location":"examples/#train-a-convolution-neural-network-on-the-concatenation-of-word-embeddings","text":"In this example we will show how to train a convolution neural network with a similar arquitecture to the one used in this paper from the Weka GUI. Represent each tweet from the sent140test.arff.gz dataset as a sequence of its first 15 word embeddings by pasting the following filter configuration: weka.filters.unsupervised.attribute.TweetToEmbeddingsFeatureVector -S CONCATENATE_ACTION -embeddingHandler \"affective.core.CSVEmbeddingHandler -K $HOME/wekafiles/packages/AffectiveTweets/resources/w2v.twitter.edinburgh.100d.csv.gz -sep TAB -I last\" -K 15 -stemmer weka.core.stemmers.NullStemmer -stopwords-handler \"weka.core.stopwords.Null \" -I 1 -U -tokenizer \"weka.core.tokenizers.TweetNLPTokenizer \" Discard the string content and move the class label to the last position: weka.filters.unsupervised.attribute.Reorder -R 4-last,3 Train a convolutional neural network using a Dl4jMlpClassifier . Paste the following snippet in the Classification panel: weka.classifiers.functions.Dl4jMlpClassifier -S 1 -iterator \"weka.dl4j.iterators.ConvolutionalInstancesIterator -height 1 -numChannels 1 -bs 256 -width 1500\" -layers \"weka.dl4j.layers.ConvolutionLayer -nFilters 100 -activation identity -adamMeanDecay 0.9 -adamVarDecay 0.999 -biasInit 1.0 -biasL1 0.0 -biasL2 0.0 -blr 0.01 -mode Truncate -cudnnAlgoMode PREFER_FASTEST -dist \\\"weka.dl4j.distribution.NormalDistribution -mean 0.001 -std 1.0\\\" -dropout 0.0 -epsilon 1.0E-6 -gradientNormalization None -gradNormThreshold 1.0 -kernelSizeX 300 -kernelSizeY 1 -L1 0.0 -L2 0.0 -name \\\"Convolution layer\\\" -lr 0.01 -momentum 0.9 -paddingX 0 -paddingY 0 -rho 0.0 -rmsDecay 0.95 -strideX 100 -strideY 1 -updater NESTEROVS -weightInit XAVIER\" -layers \"weka.dl4j.layers.OutputLayer -activation softmax -adamMeanDecay 0.9 -adamVarDecay 0.999 -biasInit 1.0 -biasL1 0.0 -biasL2 0.0 -blr 0.01 -dist \\\"weka.dl4j.distribution.NormalDistribution -mean 0.001 -std 1.0\\\" -dropout 0.0 -epsilon 1.0E-6 -gradientNormalization None -gradNormThreshold 1.0 -L1 0.0 -L2 0.0 -name \\\"Output layer\\\" -lr 0.01 -lossFn LossMCXENT() -momentum 0.9 -rho 0.0 -rmsDecay 0.95 -updater NESTEROVS -weightInit XAVIER\" -logFile weka.log -numEpochs 200 -algorithm STOCHASTIC_GRADIENT_DESCENT Note: This code is not compatible with the latest version of the WekaDeepLearning4j package. This network has 100 filters in a convolutional layer, followed by the output layer. The filter size is 300x1 (i.e, each filter maps a word trigram, since each word has 100 dimensions). The stride is 100x1 (the number of dimensions for a word). The number of epochs is 200. The input width is 1500 and the input height is 1. The number of input channels is 1 and the batch size is 256.","title":"Train a Convolution Neural Network on the concatenation of word embeddings"},{"location":"install/","text":"Installing Weka Download the latest stable version of Weka. Installing AffectiveTweets Install AffectiveTweets1.0.2 using the WekaPackageManager : java -cp $WEKA_PATH/weka.jar weka.core.WekaPackageManager -install-package AffectiveTweets Make sure you have installed version 1.0.2 to run the examples . In case of having problems with the Weka packages repository, install the package as follows: java -cp $WEKA_PATH/weka.jar weka.core.WekaPackageManager -install-package https://github.com/felipebravom/AffectiveTweets/releases/download/1.0.2/AffectiveTweets1.0.2.zip Building AffectiveTweets You can also build the package from the Github version using the project\u2019s build-package.xml file with the ant build tool. This is very useful if you want to modify the code or contribute with a new feature. # clone the repository git clone https://github.com/felipebravom/AffectiveTweets.git cd AffectiveTweets # Download additional files wget https://github.com/felipebravom/AffectiveTweets/releases/download/1.0.1/extra.zip unzip extra.zip # Build the package using apache ant ant -f build_package.xml make_package # Install the built package java -cp $WEKA_PATH/weka.jar weka.core.WekaPackageManager -install-package dist/AffectiveTweets.zip Testing AffectiveTweets The software can be tested using Junit test cases. The package must either be installed or JVM must be started in AffectiveTweets directory. # run all tests ant -f build_package.xml run_tests_all Other Useful Packages We recommend installing other useful packages for classification, regression and evaluation: LibLinear : This package is required for running the examples . java -cp $WEKA_PATH/weka.jar weka.core.WekaPackageManager -install-package LibLINEAR LibSVM java -cp $WEKA_PATH/weka.jar weka.core.WekaPackageManager -install-package LibSVM RankCorrelation java -cp $WEKA_PATH/weka.jar weka.core.WekaPackageManager -install-package RankCorrelation Snowball-stemmers : This package allows using the Porter stemmer as well as other Snowball stemmers. java -cp $WEKA_PATH/weka.jar weka.core.WekaPackageManager -install-package snowball-stemmers The WekaDeepLearning4j package can be installed for training deep neural networks and word embeddings.","title":"Installation"},{"location":"install/#installing-weka","text":"Download the latest stable version of Weka.","title":"Installing Weka"},{"location":"install/#installing-affectivetweets","text":"Install AffectiveTweets1.0.2 using the WekaPackageManager : java -cp $WEKA_PATH/weka.jar weka.core.WekaPackageManager -install-package AffectiveTweets Make sure you have installed version 1.0.2 to run the examples . In case of having problems with the Weka packages repository, install the package as follows: java -cp $WEKA_PATH/weka.jar weka.core.WekaPackageManager -install-package https://github.com/felipebravom/AffectiveTweets/releases/download/1.0.2/AffectiveTweets1.0.2.zip","title":"Installing AffectiveTweets"},{"location":"install/#building-affectivetweets","text":"You can also build the package from the Github version using the project\u2019s build-package.xml file with the ant build tool. This is very useful if you want to modify the code or contribute with a new feature. # clone the repository git clone https://github.com/felipebravom/AffectiveTweets.git cd AffectiveTweets # Download additional files wget https://github.com/felipebravom/AffectiveTweets/releases/download/1.0.1/extra.zip unzip extra.zip # Build the package using apache ant ant -f build_package.xml make_package # Install the built package java -cp $WEKA_PATH/weka.jar weka.core.WekaPackageManager -install-package dist/AffectiveTweets.zip","title":"Building AffectiveTweets"},{"location":"install/#testing-affectivetweets","text":"The software can be tested using Junit test cases. The package must either be installed or JVM must be started in AffectiveTweets directory. # run all tests ant -f build_package.xml run_tests_all","title":"Testing AffectiveTweets"},{"location":"install/#other-useful-packages","text":"We recommend installing other useful packages for classification, regression and evaluation: LibLinear : This package is required for running the examples . java -cp $WEKA_PATH/weka.jar weka.core.WekaPackageManager -install-package LibLINEAR LibSVM java -cp $WEKA_PATH/weka.jar weka.core.WekaPackageManager -install-package LibSVM RankCorrelation java -cp $WEKA_PATH/weka.jar weka.core.WekaPackageManager -install-package RankCorrelation Snowball-stemmers : This package allows using the Porter stemmer as well as other Snowball stemmers. java -cp $WEKA_PATH/weka.jar weka.core.WekaPackageManager -install-package snowball-stemmers The WekaDeepLearning4j package can be installed for training deep neural networks and word embeddings.","title":"Other Useful Packages"},{"location":"videos/","text":"The demo videos from below show how to use AffectiveTweets with the WEKA GUI. Video 1: Training sentiment classification models for tweets Click here to download the video in mp4 format. Video 2: Creating lexicons for Twitter sentiment analysis Click here to download the video in mp4 format. Video 3: Twitter sentiment classification with distant supervision Click here to download the video in mp4 format.","title":"Videos"},{"location":"videos/#video-1-training-sentiment-classification-models-for-tweets","text":"Click here to download the video in mp4 format.","title":"Video 1: Training sentiment classification models for tweets"},{"location":"videos/#video-2-creating-lexicons-for-twitter-sentiment-analysis","text":"Click here to download the video in mp4 format.","title":"Video 2: Creating lexicons for Twitter sentiment analysis"},{"location":"videos/#video-3-twitter-sentiment-classification-with-distant-supervision","text":"Click here to download the video in mp4 format.","title":"Video 3: Twitter sentiment classification with distant supervision"}]}